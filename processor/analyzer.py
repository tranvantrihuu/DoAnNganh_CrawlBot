import math
import os
import re
import unicodedata
from datetime import datetime
from pathlib import Path
from difflib import get_close_matches, SequenceMatcher
from typing import List, Dict, Tuple, Optional
import numpy as np
import pandas as pd

# C·ªë g·∫Øng d√πng _parse_dt ƒë√£ ƒë·ªãnh nghƒ©a ·ªü module preprocess (n·∫øu c√≥) ƒë·ªÉ th·ªëng nh·∫•t c√°ch parse timestamp.
# N·∫øu import th·∫•t b·∫°i (kh√°c m√¥i tr∆∞·ªùng/th∆∞ m·ª•c), fallback sang h√†m n·ªôi b·ªô b√™n d∆∞·ªõi.
try:
    from processor.preprocess import _parse_dt as _parse_dt_ext
except Exception:
    _parse_dt_ext = None

def _parse_dt(date_str: str, time_str: str) -> datetime:
    # Wrapper: ∆∞u ti√™n d√πng _parse_dt_ext n·∫øu c√≥; n·∫øu kh√¥ng th√¨ parse theo ƒë·ªãnh d·∫°ng m·∫∑c ƒë·ªãnh "YYYY-mm-dd HHMMSS".
    return _parse_dt_ext(date_str, time_str) if _parse_dt_ext else datetime.strptime(
        f"{date_str} {time_str}", "%Y-%m-%d %H%M%S"
    )

# ==== x√°c ƒë·ªãnh project root theo v·ªã tr√≠ file n√†y ====
# analyzer.py n·∫±m ·ªü: <root>/processor/analyzer.py ‚Üí parents[1] l√† th∆∞ m·ª•c root d·ª± √°n.
ROOT = Path(__file__).resolve().parents[1]

# ==== ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi ====
# Th∆∞ m·ª•c ƒë·∫ßu v√†o (s·∫£n ph·∫©m c·ªßa preprocess) v√† ƒë·∫ßu ra (k·∫øt qu·∫£ ph√¢n t√≠ch).
PREPROCESS_DIR = ROOT / "output" / "preprocess"
ANALYZER_DIR   = ROOT / "output" / "analyzer"

# ==== regex t√™n file (match c·∫£ processed & preprocessed) ====
# M·∫´u nh·∫≠n di·ªán file h·ª£p l·ªá ƒë·ªÉ ƒë∆∞a v√†o ph√¢n t√≠ch:
#   job_detail_output_<slug>_g<gid>_<loc>_<YYYY-mm-dd>_<HHMMSS>[_processed|_preprocessed].xlsx
FNAME_RE = re.compile(
    r"^job_detail_output_(?P<slug>.+?)_g(?P<gid>\d+)_(?P<loc>\d{4})_"
    r"(?P<date>\d{4}-\d{2}-\d{2})_(?P<time>\d{6})"
    r"(?:_(?P<suffix>(?:pre)?processed))?\.xlsx$",
    re.IGNORECASE,
)

DEBUG = True  # B·∫≠t LOG debug chi ti·∫øt trong qu√° tr√¨nh qu√©t file m·ªõi nh·∫•t.

def _make_analyzer_path(src: Path) -> Path:
    # T·∫°o ƒë∆∞·ªùng d·∫´n file ƒë·∫ßu ra cho m·ªôt file ngu·ªìn:
    # - Lo·∫°i b·ªè h·∫≠u t·ªë _processed/_preprocessed kh·ªèi stem.
    # - Ghi ra th∆∞ m·ª•c analyzer, t√™n ƒëu√¥i _analyzed.xlsx
    stem = src.stem
    stem = re.sub(r"_(?:pre)?processed$", "", stem, flags=re.IGNORECASE)
    out_name = f"{stem}_analyzed.xlsx"
    ANALYZER_DIR.mkdir(parents=True, exist_ok=True)
    return ANALYZER_DIR / out_name

def get_latest_detail_files(base_dir: Path) -> List[Path]:
    # Duy·ªát base_dir, group theo (slug, gid, loc) v√† ch·ªçn phi√™n b·∫£n M·ªöI NH·∫§T d·ª±a tr√™n timestamp trong t√™n file.
    latest: Dict[Tuple[str, str, str], Tuple[datetime, Path]] = {}

    if DEBUG:
        print(f"[DEBUG] CWD       : {Path.cwd()}")
        print(f"[DEBUG] base_dir  : {base_dir} (exists={base_dir.exists()})")

    if not base_dir.exists():
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c: {base_dir}")

    for p in base_dir.iterdir():
        if not p.is_file():
            continue
        m = FNAME_RE.match(p.name)
        if DEBUG:
            print(f"[DEBUG] {p.name} -> {'MATCH' if m else 'NO MATCH'}")
        if not m:
            continue

        slug = m.group("slug")
        gid  = m.group("gid")
        loc  = m.group("loc")
        dt   = _parse_dt(m.group("date"), m.group("time"))
        key  = (slug, gid, loc)
        cur  = latest.get(key)
        if cur is None or dt > cur[0]:
            latest[key] = (dt, p)

    return [info[1] for info in latest.values()]

def save_combined_with_timestamp(
    df: pd.DataFrame,
    out_dir: Path = ANALYZER_DIR,
    prefix: str = "job_detail_output__combined"
) -> Path:
    """
    Ghi file t·ªïng h·ª£p k·∫øt qu·∫£ ph√¢n t√≠ch v·ªõi timestamp v√†o t√™n:
    output/analyzer/job_detail_output__combined_{YYYY-mm-dd_HHMMSS}_analyzed.xlsx
    """
    out_dir.mkdir(parents=True, exist_ok=True)
    ts = datetime.now().strftime("%Y-%m-%d_%H%M%S")
    out_path = out_dir / f"{prefix}_{ts}_analyzed.xlsx"
    df.to_excel(out_path, index=False)
    return out_path

def _norm_text_no_accent(s) -> str:
    """Chu·∫©n ho√° vƒÉn b·∫£n: strip/ lower/ b·ªè d·∫•u (NFD ‚Äì lo·∫°i Mn). D√πng ƒë·ªÉ nh·∫≠n di·ªán 'kh√¥ng y√™u c·∫ßu' / bi·∫øn th·ªÉ."""
    if pd.isna(s):
        return ""
    t = str(s).strip().lower()
    t = "".join(c for c in unicodedata.normalize("NFD", t) if unicodedata.category(c) != "Mn")
    return t

def _to_safe_str(x):
    # √âp v·ªÅ chu·ªói an to√†n (x·ª≠ l√Ω None/NaN). Tr√°nh l·ªói khi ƒë∆∞a v√†o so kh·ªõp/ so s√°nh.
    if x is None:
        return ""
    try:
        if pd.isna(x):
            return ""
    except Exception:
        pass
    return str(x)

def safe_similarity(a, b):
    # So s√°nh ƒë·ªô t∆∞∆°ng ƒë·ªìng 2 chu·ªói (0..1) v·ªõi SequenceMatcher, c√≥ ph√≤ng ng·ª´a l·ªói & √©p chu·ªói an to√†n.
    a = _to_safe_str(a)
    b = _to_safe_str(b)
    try:
        # autojunk=False ƒë·ªÉ ·ªïn ƒë·ªãnh h∆°n v·ªõi chu·ªói ng·∫Øn/√≠t k√Ω t·ª± l·∫∑p.
        return SequenceMatcher(None, a, b, autojunk=False).ratio()
    except Exception:
        return 0.0

def safe_get_close_matches(word, possibilities, n=3, cutoff=0.6):
    # G·ª£i √Ω c·ªôt g·∫ßn ƒë√∫ng khi thi·∫øu c·ªôt m·ª•c ti√™u (v√≠ d·ª•: 'nganh' b·ªã vi·∫øt nh·∫ßm).
    word = _to_safe_str(word)
    poss = [_to_safe_str(p) for p in possibilities if _to_safe_str(p)]
    try:
        return get_close_matches(word, poss, n=n, cutoff=cutoff)
    except Exception:
        return []

def analyze_one_file(file_path: Path) -> Path:
    """Ph√¢n t√≠ch m·ªôt file ƒë√£ preprocess v√† ghi ra output/analyzer/<t√™n_g·ªëc>_analyzed.xlsx"""
    if not file_path.exists():
        raise SystemExit(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {file_path}")

    out_phantich = _make_analyzer_path(file_path)

    # ==== ƒê·ªåC D·ªÆ LI·ªÜU ====
    df = pd.read_excel(file_path, engine="openpyxl")
    print(f"‚úÖ ƒê·ªçc: {file_path.name} ‚Äî {df.shape[0]} d√≤ng √ó {df.shape[1]} c·ªôt")

    # === 1) In s·ªë d√≤ng/c·ªôt ===
    print(f"üìä K√≠ch th∆∞·ªõc ban ƒë·∫ßu: {df.shape[0]} d√≤ng √ó {df.shape[1]} c·ªôt")

    # === 2) X√≥a d√≤ng tr√πng ho√†n to√†n ===
    # D√πng duplicated() ƒë·ªÉ ph√°t hi·ªán h√†ng gi·ªëng 100% tr√™n m·ªçi c·ªôt; ch·ªâ gi·ªØ b·∫£n ghi ƒë·∫ßu ti√™n.
    dup_exact = df.duplicated(keep="first").sum()
    if dup_exact > 0:
        df = df.drop_duplicates(keep="first").reset_index(drop=True)
        print(f"üóëÔ∏è  ƒê√£ x√≥a {dup_exact} d√≤ng tr√πng ho√†n to√†n. C√≤n l·∫°i: {len(df)} d√≤ng")
    else:
        print("‚úÖ Kh√¥ng c√≥ d√≤ng tr√πng ho√†n to√†n.")

    # === 3) X√≥a d√≤ng tr√πng x·∫•p x·ªâ (‚â• 90%) ===
    # √ù t∆∞·ªüng:
    #  - Chu·∫©n ho√° text t·ª´ng √¥ (lower/strip/lo·∫°i 'nan'/'no_info') ‚Üí gh√©p to√†n h√†ng th√†nh 1 chu·ªói.
    #  - Bucket theo ƒë·ªô d√†i (len // 20) ƒë·ªÉ gi·∫£m s·ªë c·∫∑p so s√°nh O(n¬≤).
    #  - D√πng SequenceMatcher.ratio() cho t·ª´ng c·∫∑p trong c√πng bucket, ng∆∞·ª°ng 0.90.
    limit = min(len(df), 5000)  # gi·ªõi h·∫°n ƒë·ªÉ tr√°nh O(n¬≤) qu√° n·∫∑ng

    # Chu·∫©n h√≥a t·ª´ng √¥ -> chu·ªói g·ªçn g√†ng ƒë·ªÉ so s√°nh
    df_cmp = df.iloc[:limit].copy()
    for c in df_cmp.columns:
        s = df_cmp[c].astype(str).str.strip().str.lower()
        s = s.mask(s.isin(["nan", "no_info"]), "")
        s = s.str.replace(r"\s+", " ", regex=True)
        df_cmp[c] = s

    # Gh√©p m·ªói d√≤ng th√†nh m·ªôt chu·ªói duy nh·∫•t (lo·∫°i √¥ r·ªóng ƒë·ªÉ gi·∫£m nhi·ªÖu).
    row_texts = df_cmp.apply(lambda r: " | ".join([v for v in r.tolist() if v]), axis=1).tolist()

    # Nh√≥m th√¥ theo ƒë·ªô d√†i chu·ªói ƒë·ªÉ gi·∫£m s·ªë c·∫∑p so s√°nh (bucketization).
    bucket_map: Dict[int, list] = {}
    for i, t in enumerate(row_texts):
        L = len(t) // 20
        bucket_map.setdefault(L, []).append(i)

    # So s√°nh trong t·ª´ng bucket, ƒë√°nh d·∫•u c√°c v·ªã tr√≠ c·∫ßn xo√° (gi·ªØ d√≤ng ƒë·∫ßu ti√™n).
    to_drop_pos = set()
    for _, idxs in bucket_map.items():
        m = len(idxs)
        for a in range(m):
            i = idxs[a]
            if i in to_drop_pos:
                continue
            ti = row_texts[i]
            if not ti:
                continue
            for b in range(a + 1, m):
                j = idxs[b]
                if j in to_drop_pos:
                    continue
                tj = row_texts[j]
                if not tj:
                    continue
                sim = SequenceMatcher(None, ti, tj).ratio()
                if sim >= 0.90:
                    to_drop_pos.add(j)  # b·ªè d√≤ng sau, gi·ªØ d√≤ng ƒë·∫ßu

    # Map t·ª´ v·ªã tr√≠ so s√°nh (iloc) sang index th·∫≠t r·ªìi drop.
    approx_drop_idx = df.iloc[list(to_drop_pos)].index.tolist()
    if approx_drop_idx:
        print(f"üóëÔ∏è  Ph√°t hi·ªán {len(approx_drop_idx)} d√≤ng tr√πng x·∫•p x·ªâ (‚â•90%), ti·∫øn h√†nh x√≥a.")
        df = df.drop(index=approx_drop_idx).reset_index(drop=True)
        print(f"‚úÖ Sau khi x√≥a tr√πng x·∫•p x·ªâ: {len(df)} d√≤ng")
    else:
        print("‚úÖ Kh√¥ng ph√°t hi·ªán d√≤ng tr√πng x·∫•p x·ªâ.")

    # === 4) K·∫øt qu·∫£ cu·ªëi c√πng ===
    print(f"üìä K√≠ch th∆∞·ªõc cu·ªëi c√πng: {df.shape[0]} d√≤ng √ó {df.shape[1]} c·ªôt")

    # Chu·∫©n b·ªã th·ªëng k√™ theo 'nganh'
    TARGET_COL = "nganh"
    if TARGET_COL not in df.columns:
        # N·∫øu thi·∫øu c·ªôt, g·ª£i √Ω c√°c t√™n g·∫ßn ƒë√∫ng ƒë·ªÉ d·ªÖ s·ª≠a pipeline/ƒë·∫ßu v√†o.
        suggestion = get_close_matches(TARGET_COL, df.columns.tolist(), n=3, cutoff=0.6)
        raise SystemExit(f"‚ùå Kh√¥ng t√¨m th·∫•y c·ªôt '{TARGET_COL}'. G·ª£i √Ω: {', '.join(suggestion)}")

    # C·ªôt ng√†nh g·ªëc (gi·ªØ d·∫•u/hoa-th∆∞·ªùng nh∆∞ file), thay √¥ r·ªóng b·∫±ng "no_info"
    s_nganh_raw = df[TARGET_COL].astype(str).str.strip()
    s_nganh_raw = s_nganh_raw.replace(r"^\s*$", "no_info", regex=True)

    # C·ªôt ng√†nh chu·∫©n ho√° (lower) ƒë·ªÉ gom nh√≥m khi value kh√¥ng ƒë·ªìng nh·∫•t ki·ªÉu ch·ªØ.
    s_nganh_norm = s_nganh_raw.str.lower()

    # √Ånh x·∫° "d·∫°ng chu·∫©n ho√°" ‚Üí "b·∫£n g·ªëc ƒë·∫ßu ti√™n" ƒë·ªÉ sau g·ªôp nh√≥m v·∫´n hi·ªÉn th·ªã t√™n ƒë·∫πp c√≥ d·∫•u.
    map_norm_to_raw = {}
    for norm, raw in zip(s_nganh_norm, s_nganh_raw):
        if norm not in map_norm_to_raw:
            map_norm_to_raw[norm] = raw

    # Chu·∫©n ho√° c·ªù 'check_luong' v·ªÅ bool; n·∫øu thi·∫øu c·ªôt th√¨ coi nh∆∞ False to√†n b·ªô.
    has_check = "check_luong" in df.columns
    if has_check:
        def _to_bool(x):
            if isinstance(x, bool):
                return x
            s = str(x).strip().lower()
            if s in {"true", "1", "yes", "y"}:
                return True
            if s in {"false", "0", "no", "n"}:
                return False
            return False

        s_check = df["check_luong"].map(_to_bool)
    else:
        s_check = pd.Series([False] * len(df))

    # ƒê·∫øm s·ªë tin theo ng√†nh
    counts = s_nganh_norm.value_counts(dropna=False)
    total = int(counts.sum())

    # ƒê·∫øm theo ng√†nh t√°ch theo c√≥/kh√¥ng c√≥ l∆∞∆°ng (d·ª±a tr√™n check_luong)
    by_nganh_check = (
        pd.DataFrame({TARGET_COL: s_nganh_norm, "_check": s_check})
        .groupby(TARGET_COL, dropna=False)["_check"]
        .agg(
            so_tin_co_luong=lambda x: int(x.sum()),
            so_tin_co_thuong_luong=lambda x: int((~x).sum())
        )
        .reset_index()
    )

    # G·ªôp 2 b·∫£ng l·∫°i th√†nh th·ªëng k√™ ch√≠nh
    stats_df = (
        counts.rename("so_tin").to_frame()
        .reset_index()
        .rename(columns={"index": TARGET_COL})
        .merge(by_nganh_check, how="left", on=TARGET_COL)
    )

    # ƒê·ªïi kho√° chu·∫©n ho√° v·ªÅ nh√£n g·ªëc c√≥ d·∫•u ƒë·ªÉ hi·ªÉn th·ªã ƒë·∫πp
    stats_df[TARGET_COL] = stats_df[TARGET_COL].map(map_norm_to_raw)

    # ƒêi·ªÅn 0 cho c·ªôt ƒë·∫øm n·∫øu thi·∫øu (an to√†n khi merge)
    for c in ["so_tin_co_luong", "so_tin_co_thuong_luong"]:
        if c in stats_df.columns:
            stats_df[c] = stats_df[c].fillna(0).astype("Int64")
        else:
            stats_df[c] = pd.Series([0] * len(stats_df), dtype="Int64")

    # T√≠nh t·ª∑ l·ªá %
    stats_df["ty_le"] = (stats_df["so_tin"] / total * 100).round(2)

    # S·∫Øp x·∫øp ng√†nh theo s·ªë tin gi·∫£m d·∫ßn
    stats_df = stats_df.sort_values("so_tin", ascending=False, ignore_index=True)

    # Th√™m d√≤ng t·ªïng c·ªông cu·ªëi b·∫£ng
    row_total = pd.DataFrame([{
        TARGET_COL: "T·ªïng c·ªông",
        "so_tin": stats_df["so_tin"].sum(),
        "ty_le": 100.0,
        "so_tin_co_luong": stats_df["so_tin_co_luong"].sum(skipna=True),
        "so_tin_co_thuong_luong": stats_df["so_tin_co_thuong_luong"].sum(skipna=True)
    }])

    stats_df_out = pd.concat([stats_df, row_total], ignore_index=True)

    # Ghi k·∫øt qu·∫£ ra m·ªôt sheet duy nh·∫•t (tin_theo_nganh) trong file analyzed
    with pd.ExcelWriter(out_phantich, engine="openpyxl", mode="w") as writer:
        stats_df_out.to_excel(writer, sheet_name="tin_theo_nganh", index=False)

    print(f"üìÑ ƒê√£ l∆∞u {len(stats_df_out)} d√≤ng (sheet: 'tin_theo_nganh').")

    # ==== 5) L∆∞∆°ng trung b√¨nh & min/max theo ng√†nh ====
    SALARY_COL = "med_luong"  # C·ªôt l∆∞∆°ng ƒë√£ ƒë∆∞·ª£c chu·∫©n ho√° v·ªÅ VND/th√°ng (gi√° tr·ªã "trung v·ªã/median" cho m·ªói b·∫£n ghi)
    MIN_COL = "min_luong"  # C·ªôt l∆∞∆°ng t·ªëi thi·ªÉu (tu·ª≥ ch·ªçn, c√≥ th·ªÉ kh√¥ng t·ªìn t·∫°i)
    MAX_COL = "max_luong"  # C·ªôt l∆∞∆°ng t·ªëi ƒëa (tu·ª≥ ch·ªçn, c√≥ th·ªÉ kh√¥ng t·ªìn t·∫°i)

    # Ki·ªÉm tra c·ªôt b·∫Øt bu·ªôc (ph·∫£i c√≥ c·ªôt ng√†nh + c·ªôt l∆∞∆°ng chu·∫©n ho√°)
    for col_need in [TARGET_COL, SALARY_COL]:
        if col_need not in df.columns:
            sug = get_close_matches(col_need, df.columns.tolist(), n=3, cutoff=0.6)
            raise SystemExit(f"‚ùå Thi·∫øu c·ªôt '{col_need}'. G·ª£i √Ω: {', '.join(sug) if sug else 'kh√¥ng c√≥'}")

    # √âp c·ªôt l∆∞∆°ng v·ªÅ ki·ªÉu s·ªë (nan n·∫øu kh√¥ng chuy·ªÉn ƒë∆∞·ª£c)
    s_med = pd.to_numeric(df[SALARY_COL], errors="coerce")

    # Hai c·ªôt min/max c√≥ th·ªÉ kh√¥ng t·ªìn t·∫°i -> t·∫°o Series m·ªÅm ƒë·ªÉ x·ª≠ l√Ω chung
    has_min = MIN_COL in df.columns
    has_max = MAX_COL in df.columns
    s_min = pd.to_numeric(df[MIN_COL], errors="coerce") if has_min else pd.Series([pd.NA] * len(df))
    s_max = pd.to_numeric(df[MAX_COL], errors="coerce") if has_max else pd.Series([pd.NA] * len(df))

    # T√≠nh trung b√¨nh to√†n b·ªô (round ƒë·∫øn ngh√¨n cho d·ªÖ ƒë·ªçc); n·∫øu kh√¥ng c√≥ d·ªØ li·ªáu th√¨ g√°n NaN
    overall_mean = float(s_med.mean()) if s_med.notna().any() else np.nan
    tb_chung = None if not np.isfinite(overall_mean) else round(overall_mean, -3)

    # Th√™m: min to√†n t·∫≠p v√† max to√†n t·∫≠p (n·∫øu c√≥ d·ªØ li·ªáu min/max)
    global_min_min = None
    global_max_max = None
    if has_min and s_min.notna().any():
        global_min_min = int(round(float(s_min.min(skipna=True)), -3))
    if has_max and s_max.notna().any():
        global_max_max = int(round(float(s_max.max(skipna=True)), -3))

    # C·ªù l∆∞∆°ng b·∫•t th∆∞·ªùng: ƒë√°nh d·∫•u c√°c d√≤ng c√≥ med_luong == "bat_thuong" (x·ª≠ l√Ω an to√†n d√π c·ªôt l√† s·ªë/chu·ªói)
    s_bat = df[SALARY_COL].astype(str).str.strip().str.lower().eq("bat_thuong")

    # Chu·∫©n b·ªã tr∆∞·ªùng "chi ti·∫øt b·∫•t th∆∞·ªùng" (g·ªôp luong | href n·∫øu c√≥)
    has_luong = "luong" in df.columns
    has_href = "href" in df.columns

    def _fmt_detail(idx, row):
        if not s_bat.iat[idx]:
            return None
        parts = []
        if has_luong:
            v = str(row["luong"])
            if v and v.strip() and v.strip().lower() != "nan":
                parts.append(v.strip())
        if has_href:
            v = str(row["href"])
            if v and v.strip() and v.strip().lower() != "nan":
                parts.append(v.strip())
        return " | ".join(parts) if parts else None

    detail_series = df.apply(lambda r: _fmt_detail(r.name, r), axis=1)

    # ƒê·∫øm s·ªë tin b·∫•t th∆∞·ªùng + gom m√¥ t·∫£ chi ti·∫øt theo ng√†nh
    bat_detail_df = (
        df.assign(_bat=s_bat, _detail=detail_series)
        .groupby(TARGET_COL, dropna=False)
        .agg(
            so_tin_luong_bat_thuong=("_bat", lambda x: int(x.sum())),
            chi_tiet_bat_thuong=("_detail", lambda col: "khong_phat_hien"
            if col.dropna().empty else "; ".join(map(str, col.dropna().tolist())))
        )
        .reset_index()
    )

    # T√≠nh TB/min/max theo ng√†nh (d·ª±a tr√™n c√°c c·ªôt *_num ƒë√£ √©p ki·ªÉu)
    agg_df = (
        df.assign(
            med_luong_num=s_med,
            min_luong_num=s_min,
            max_luong_num=s_max
        )
        .groupby(TARGET_COL, dropna=False)
        .agg(
            med_luong_num=('med_luong_num', 'mean'),
            min_luong=('min_luong_num', 'min') if has_min else ('med_luong_num', lambda x: pd.NA),
            max_luong=('max_luong_num', 'max') if has_max else ('med_luong_num', lambda x: pd.NA),
        )
        .dropna(subset=['med_luong_num'])
        .reset_index()
    )

    # L√†m tr√≤n ƒë·∫øn ngh√¨n & chu·∫©n ho√° ki·ªÉu Int64 (c√≥ th·ªÉ ch·ª©a NA)
    agg_df["med_luong_tb"] = agg_df["med_luong_num"].round(-3).astype("Int64")
    if has_min:
        agg_df["min_luong"] = agg_df["min_luong"].round(-3).astype("Int64")
    else:
        agg_df["min_luong"] = pd.Series([pd.NA] * len(agg_df), dtype="Int64")

    if has_max:
        agg_df["max_luong"] = agg_df["max_luong"].round(-3).astype("Int64")
    else:
        agg_df["max_luong"] = pd.Series([pd.NA] * len(agg_df), dtype="Int64")

    # So s√°nh v·ªõi TB chung (ƒë∆°n v·ªã: %); n·∫øu kh√¥ng c√≥ TB chung th√¨ g√°n 0.0
    if np.isfinite(overall_mean) and overall_mean > 0:
        agg_df["so_voi_tb_chung"] = ((agg_df["med_luong_num"] - overall_mean) / overall_mean * 100).round(1)
    else:
        agg_df["so_voi_tb_chung"] = 0.0

    # G·ªôp th√™m s·ªë tin & chi ti·∫øt b·∫•t th∆∞·ªùng theo ng√†nh
    agg_merged = agg_df.merge(bat_detail_df, how="left", on=TARGET_COL)
    agg_merged["so_tin_luong_bat_thuong"] = agg_merged["so_tin_luong_bat_thuong"].fillna(0).astype("Int64")
    agg_merged["chi_tiet_bat_thuong"] = agg_merged["chi_tiet_bat_thuong"].fillna("khong_phat_hien")

    # B·∫£ng cu·ªëi c√πng cho sheet l∆∞∆°ng
    # (KH√îNG th√™m c·ªôt ƒë·∫øm "so_tin_co_luong" / "so_tin_co_thuong_luong")
    salary_df = (
        agg_merged.sort_values("med_luong_tb", ascending=False)
        .rename(columns={TARGET_COL: "nganh"})
        [["nganh", "med_luong_tb", "min_luong", "max_luong", "so_voi_tb_chung",
          "so_tin_luong_bat_thuong", "chi_tiet_bat_thuong"]]
        .reset_index(drop=True)
    )

    # Th√™m d√≤ng "T·ªïng quan (TB)" n·∫øu t√≠nh ƒë∆∞·ª£c TB chung
    # (k√®m min/max to√†n t·∫≠p, t·ªïng s·ªë d√≤ng b·∫•t th∆∞·ªùng; "chi_tiet_bat_thuong" ƒë·ªÉ NA)
    if tb_chung is not None:
        total_bat = int(s_bat.sum(skipna=True))
        row_tong = {
            "nganh": "T·ªïng quan (TB)",
            "med_luong_tb": int(tb_chung),
            "min_luong": (pd.NA if global_min_min is None else pd.array([global_min_min], dtype="Int64")[0]),
            "max_luong": (pd.NA if global_max_max is None else pd.array([global_max_max], dtype="Int64")[0]),
            "so_voi_tb_chung": 0.0,
            "so_tin_luong_bat_thuong": pd.NA if total_bat is None else total_bat,
            "chi_tiet_bat_thuong": pd.NA
        }
        salary_df = pd.concat([salary_df, pd.DataFrame([row_tong])], ignore_index=True)

        # Ghi sheet Excel "phan_tich_luong" (append n·∫øu file ƒë√£ t·ªìn t·∫°i, thay th·∫ø sheet n·∫øu c√≥ s·∫µn)
        sheet_salary = "phan_tich_luong"
        try:
            if out_phantich.exists():
                with pd.ExcelWriter(out_phantich, engine="openpyxl", mode="a", if_sheet_exists="replace") as writer:
                    salary_df.to_excel(writer, sheet_name=sheet_salary, index=False)
            else:
                with pd.ExcelWriter(out_phantich, engine="openpyxl", mode="w") as writer:
                    salary_df.to_excel(writer, sheet_name=sheet_salary, index=False)

            # Log t√≥m t·∫Øt sau khi ghi
            print(f"üìÑ ƒê√£ l∆∞u {len(salary_df)} d√≤ng (sheet: '{sheet_salary}').")
            if tb_chung is not None:
                print(f"‚Ä¢ L∆∞∆°ng TB to√†n b·ªô: {tb_chung:,.0f} VND/th√°ng")
            if has_min and (global_min_min is not None):
                print(f"‚Ä¢ Min nh·ªè nh·∫•t to√†n t·∫≠p: {global_min_min:,.0f} VND/th√°ng")
            if has_max and (global_max_max is not None):
                print(f"‚Ä¢ Max l·ªõn nh·∫•t to√†n t·∫≠p: {global_max_max:,.0f} VND/th√°ng")
            if not has_min:
                print("‚ö†Ô∏è Kh√¥ng th·∫•y c·ªôt 'min_luong' ‚Üí c·ªôt 'min_luong' trong sheet s·∫Ω l√† NA.")
            if not has_max:
                print("‚ö†Ô∏è Kh√¥ng th·∫•y c·ªôt 'max_luong' ‚Üí c·ªôt 'max_luong' trong sheet s·∫Ω l√† NA.")
        except Exception as e:
            # B·∫Øt l·ªói khi ghi file Excel
            print(f"‚ùå L·ªói khi ghi Excel (sheet l∆∞∆°ng): {e}")

        # ==== 6) Th·ªëng k√™ y√™u c·∫ßu kinh nghi·ªám theo ng√†nh ====
        EXP_COL = "so_nam_kinh_nghiem"
        if EXP_COL not in df.columns or TARGET_COL not in df.columns:
            print("‚ö†Ô∏è B·ªè qua th·ªëng k√™ kinh nghi·ªám (thi·∫øu c·ªôt).")
            return out_phantich

        # --- Helpers ---
        def _safe_str(x):
            return "" if pd.isna(x) else str(x)

        def _to_num_years(x):
            s = _safe_str(x)
            if not s:
                return None
            s_norm = _norm_text_no_accent(s)  # H√†m chu·∫©n ho√° kh√¥ng d·∫•u/spacing (ƒë√£ ƒë·ªãnh nghƒ©a ·ªü ph·∫ßn tr∆∞·ªõc)
            s_norm = s_norm.replace(",", ".")  # Chu·∫©n ho√° ph√¢n s·ªë ki·ªÉu '1,5' -> '1.5'

            # B·∫Øt nhanh m·ªôt v√†i c·ª•m ph·ªï bi·∫øn (di·ªÖn gi·∫£i th√¥ ‚Üí s·ªë)
            if "duoi" in s_norm and "1" in s_norm:
                return 0.5
            if "tren" in s_norm and "7" in s_norm:
                return 7.0
            if "khong yeu cau" in s_norm:
                return None  # Tr·∫£ None: s·∫Ω ƒë∆∞·ª£c ph√¢n lo·∫°i b·∫±ng c·ªù "kh√¥ng y√™u c·∫ßu" ri√™ng

            # Tr√≠ch s·ªë (h·ªó tr·ª£ kho·∫£ng 1-3, 4‚Äì6, ‚Ä¶)
            nums = re.findall(r"\d+(?:\.\d+)?", s_norm)
            if not nums:
                # Tr∆∞·ªùng h·ª£p d·∫°ng '7+' ho·∫∑c '>=7' ho·∫∑c 'lon hon/ tren 7'
                if re.search(r"(?:\+|>=?|lon hon|tren)\s*7", s_norm):
                    return 7.0
                return None

            vals = [float(n) for n in nums]
            # N·∫øu c√≥ k√Ω hi·ªáu ng∆∞·ª°ng (>=, +) v√† gi√° tr·ªã l·ªõn, coi nh∆∞ gi√° tr·ªã l·ªõn nh·∫•t
            if re.search(r"\+|>=?", s_norm) and max(vals) >= 7:
                return max(vals)

            # N·∫øu l√† kho·∫£ng (range) ‚Üí l·∫•y trung b√¨nh (v√≠ d·ª• 1-3 ‚Üí 2.0)
            return sum(vals) / len(vals)

        # C·ªù vƒÉn b·∫£n: "kh√¥ng y√™u c·∫ßu" (no-exp) v√† ng∆∞·ª£c l·∫°i l√† "c√≥ y√™u c·∫ßu"
        df["_no_exp_flag"] = df[EXP_COL].map(lambda x: 1 if "khong yeu cau" in _norm_text_no_accent(x) else 0)
        df["_has_exp_flag"] = (df["_no_exp_flag"] == 0).astype(int)

        # Suy ra s·ªë nƒÉm kinh nghi·ªám chu·∫©n ho√° (float) ch·ªâ cho c√°c b·∫£n ghi c√≥ y√™u c·∫ßu
        df["_exp_years"] = df.apply(lambda r: (_to_num_years(r[EXP_COL]) if r["_has_exp_flag"] == 1 else None), axis=1)

        # ƒê·∫∑t c√°c "bucket" theo nƒÉm (ch·ªâ khi c√≥ gi√° tr·ªã s·ªë h·ª£p l·ªá)
        df["_b_duoi_1"] = df.apply(
            lambda r: int((r["_has_exp_flag"] == 1) and (pd.notna(r["_exp_years"])) and (r["_exp_years"] <= 1.0)),
            axis=1)
        df["_b_1_3"] = df.apply(
            lambda r: int((r["_has_exp_flag"] == 1) and (pd.notna(r["_exp_years"])) and (1.0 < r["_exp_years"] <= 3.0)),
            axis=1)
        df["_b_4_6"] = df.apply(
            lambda r: int((r["_has_exp_flag"] == 1) and (pd.notna(r["_exp_years"])) and (3.0 < r["_exp_years"] <= 6.0)),
            axis=1)
        df["_b_tren_7"] = df.apply(
            lambda r: int((r["_has_exp_flag"] == 1) and (pd.notna(r["_exp_years"])) and (r["_exp_years"] > 6.0)),
            axis=1)

        # S·ª≠ d·ª•ng _exp_years (float) ƒë·ªÉ t√≠nh mean/min/max (ch·ªâ c√°c b·∫£n ghi c√≥ y√™u c·∫ßu)
        s_exp = pd.to_numeric(df["_exp_years"], errors="coerce")

        # Gom theo ng√†nh v√† t√≠nh c√°c th·ªëng k√™ + ƒë·∫øm bucket
        exp_stats = (
            df.assign(exp_num=s_exp)
            .groupby(TARGET_COL, dropna=False)
            .agg(
                mean_exp=("exp_num", lambda x: round(x.mean(skipna=True), 2)),
                min_exp=("exp_num", "min"),
                max_exp=("exp_num", "max"),
                so_tin_no_exp=("_no_exp_flag", "sum"),
                so_tin_co_exp=("_has_exp_flag", "sum"),
                Duoi_1=("_b_duoi_1", "sum"),
                **{"1_3": ("_b_1_3", "sum")},
                **{"4_6": ("_b_4_6", "sum")},
                Tren_7=("_b_tren_7", "sum"),
            )
            .reset_index()
        )

        # √âp ki·ªÉu Int64 cho c√°c c·ªôt ƒë·∫øm (h·ªó tr·ª£ NA an to√†n h∆°n int th∆∞·ªùng)
        for c in ["so_tin_no_exp", "so_tin_co_exp", "Duoi_1", "1_3", "4_6", "Tren_7"]:
            if c in exp_stats.columns:
                exp_stats[c] = exp_stats[c].astype("Int64")

        # S·∫Øp x·∫øp k·∫øt qu·∫£: ∆∞u ti√™n ng√†nh c√≥ mean_exp cao ‚Üí sau ƒë√≥ theo t√™n ng√†nh (ASC)
        exp_stats = exp_stats.sort_values(["mean_exp", TARGET_COL], ascending=[False, True],
                                          na_position="last", ignore_index=True)

        # Th√™m d√≤ng "T·ªîNG QUAN" ·ªü cu·ªëi: t·ªïng c√°c c·ªôt ƒë·∫øm + mean/min/max t·ªïng th·ªÉ
        summary_row = {
            TARGET_COL: "T·ªîNG QUAN",
            "mean_exp": round(exp_stats["mean_exp"].mean(skipna=True), 2),
            "min_exp": exp_stats["min_exp"].min(skipna=True),
            "max_exp": exp_stats["max_exp"].max(skipna=True),
            "so_tin_no_exp": int(exp_stats["so_tin_no_exp"].sum(skipna=True)),
            "so_tin_co_exp": int(exp_stats["so_tin_co_exp"].sum(skipna=True)),
            "Duoi_1": int(exp_stats["Duoi_1"].sum(skipna=True)),
            "1_3": int(exp_stats["1_3"].sum(skipna=True)),
            "4_6": int(exp_stats["4_6"].sum(skipna=True)),
            "Tren_7": int(exp_stats["Tren_7"].sum(skipna=True)),
        }
        exp_stats = pd.concat([exp_stats, pd.DataFrame([summary_row])], ignore_index=True)

        # Ghi ra sheet Excel "nam_kinh_nghiem" (thay th·∫ø sheet n·∫øu ƒë√£ t·ªìn t·∫°i)
        try:
            with pd.ExcelWriter(out_phantich, engine="openpyxl", mode="a", if_sheet_exists="replace") as writer:
                exp_stats.to_excel(writer, sheet_name="nam_kinh_nghiem", index=False)
            print(f"üìÑ ƒê√£ l∆∞u {len(exp_stats)} d√≤ng (sheet: 'nam_kinh_nghiem').")
        except Exception as e:
            print(f"‚ùå L·ªói khi ghi Excel (sheet kinh nghi·ªám): {e}")
        # ===============================================================================
        EXP_COL = "so_nam_kinh_nghiem"
        if EXP_COL not in df.columns or TARGET_COL not in df.columns:
            print("‚ö†Ô∏è B·ªè qua th·ªëng k√™ kinh nghi·ªám (thi·∫øu c·ªôt).")
            return out_phantich

        # --- Helpers ---
        def _safe_str(x):
            return "" if pd.isna(x) else str(x)

        def _to_num_years(x):
            s = _safe_str(x)
            if not s:
                return None
            s_norm = _norm_text_no_accent(s)  # H√†m chu·∫©n ho√° kh√¥ng d·∫•u/spacing (ƒë√£ ƒë·ªãnh nghƒ©a ·ªü ph·∫ßn tr∆∞·ªõc)
            s_norm = s_norm.replace(",", ".")  # Chu·∫©n ho√° ph√¢n s·ªë ki·ªÉu '1,5' -> '1.5'

            # B·∫Øt nhanh m·ªôt v√†i c·ª•m ph·ªï bi·∫øn (di·ªÖn gi·∫£i th√¥ ‚Üí s·ªë)
            if "duoi" in s_norm and "1" in s_norm:
                return 0.5
            if "tren" in s_norm and "7" in s_norm:
                return 7.0
            if "khong yeu cau" in s_norm:
                return None  # Tr·∫£ None: s·∫Ω ƒë∆∞·ª£c ph√¢n lo·∫°i b·∫±ng c·ªù "kh√¥ng y√™u c·∫ßu" ri√™ng

            # Tr√≠ch s·ªë (h·ªó tr·ª£ kho·∫£ng 1-3, 4‚Äì6, ‚Ä¶)
            nums = re.findall(r"\d+(?:\.\d+)?", s_norm)
            if not nums:
                # Tr∆∞·ªùng h·ª£p d·∫°ng '7+' ho·∫∑c '>=7' ho·∫∑c 'lon hon/ tren 7'
                if re.search(r"(?:\+|>=?|lon hon|tren)\s*7", s_norm):
                    return 7.0
                return None

            vals = [float(n) for n in nums]
            # N·∫øu c√≥ k√Ω hi·ªáu ng∆∞·ª°ng (>=, +) v√† gi√° tr·ªã l·ªõn, coi nh∆∞ gi√° tr·ªã l·ªõn nh·∫•t
            if re.search(r"\+|>=?", s_norm) and max(vals) >= 7:
                return max(vals)

            # N·∫øu l√† kho·∫£ng (range) ‚Üí l·∫•y trung b√¨nh (v√≠ d·ª• 1-3 ‚Üí 2.0)
            return sum(vals) / len(vals)

        # C·ªù vƒÉn b·∫£n: "kh√¥ng y√™u c·∫ßu" (no-exp) v√† ng∆∞·ª£c l·∫°i l√† "c√≥ y√™u c·∫ßu"
        df["_no_exp_flag"] = df[EXP_COL].map(lambda x: 1 if "khong yeu cau" in _norm_text_no_accent(x) else 0)
        df["_has_exp_flag"] = (df["_no_exp_flag"] == 0).astype(int)

        # Suy ra s·ªë nƒÉm kinh nghi·ªám chu·∫©n ho√° (float) ch·ªâ cho c√°c b·∫£n ghi c√≥ y√™u c·∫ßu
        df["_exp_years"] = df.apply(lambda r: (_to_num_years(r[EXP_COL]) if r["_has_exp_flag"] == 1 else None), axis=1)

        # ƒê·∫∑t c√°c "bucket" theo nƒÉm (ch·ªâ khi c√≥ gi√° tr·ªã s·ªë h·ª£p l·ªá)
        df["_b_duoi_1"] = df.apply(
            lambda r: int((r["_has_exp_flag"] == 1) and (pd.notna(r["_exp_years"])) and (r["_exp_years"] <= 1.0)),
            axis=1)
        df["_b_1_3"] = df.apply(
            lambda r: int((r["_has_exp_flag"] == 1) and (pd.notna(r["_exp_years"])) and (1.0 < r["_exp_years"] <= 3.0)),
            axis=1)
        df["_b_4_6"] = df.apply(
            lambda r: int((r["_has_exp_flag"] == 1) and (pd.notna(r["_exp_years"])) and (3.0 < r["_exp_years"] <= 6.0)),
            axis=1)
        df["_b_tren_7"] = df.apply(
            lambda r: int((r["_has_exp_flag"] == 1) and (pd.notna(r["_exp_years"])) and (r["_exp_years"] > 6.0)),
            axis=1)

        # S·ª≠ d·ª•ng _exp_years (float) ƒë·ªÉ t√≠nh mean/min/max (ch·ªâ c√°c b·∫£n ghi c√≥ y√™u c·∫ßu)
        s_exp = pd.to_numeric(df["_exp_years"], errors="coerce")

        # Gom theo ng√†nh v√† t√≠nh c√°c th·ªëng k√™ + ƒë·∫øm bucket
        exp_stats = (
            df.assign(exp_num=s_exp)
            .groupby(TARGET_COL, dropna=False)
            .agg(
                mean_exp=("exp_num", lambda x: round(x.mean(skipna=True), 2)),
                min_exp=("exp_num", "min"),
                max_exp=("exp_num", "max"),
                so_tin_no_exp=("_no_exp_flag", "sum"),
                so_tin_co_exp=("_has_exp_flag", "sum"),
                Duoi_1=("_b_duoi_1", "sum"),
                **{"1_3": ("_b_1_3", "sum")},
                **{"4_6": ("_b_4_6", "sum")},
                Tren_7=("_b_tren_7", "sum"),
            )
            .reset_index()
        )

        # √âp ki·ªÉu Int64 cho c√°c c·ªôt ƒë·∫øm (h·ªó tr·ª£ NA an to√†n h∆°n int th∆∞·ªùng)
        for c in ["so_tin_no_exp", "so_tin_co_exp", "Duoi_1", "1_3", "4_6", "Tren_7"]:
            if c in exp_stats.columns:
                exp_stats[c] = exp_stats[c].astype("Int64")

        # S·∫Øp x·∫øp k·∫øt qu·∫£: ∆∞u ti√™n ng√†nh c√≥ mean_exp cao ‚Üí sau ƒë√≥ theo t√™n ng√†nh (ASC)
        exp_stats = exp_stats.sort_values(["mean_exp", TARGET_COL], ascending=[False, True],
                                          na_position="last", ignore_index=True)

        # Th√™m d√≤ng "T·ªîNG QUAN" ·ªü cu·ªëi: t·ªïng c√°c c·ªôt ƒë·∫øm + mean/min/max t·ªïng th·ªÉ
        summary_row = {
            TARGET_COL: "T·ªîNG QUAN",
            "mean_exp": round(exp_stats["mean_exp"].mean(skipna=True), 2),
            "min_exp": exp_stats["min_exp"].min(skipna=True),
            "max_exp": exp_stats["max_exp"].max(skipna=True),
            "so_tin_no_exp": int(exp_stats["so_tin_no_exp"].sum(skipna=True)),
            "so_tin_co_exp": int(exp_stats["so_tin_co_exp"].sum(skipna=True)),
            "Duoi_1": int(exp_stats["Duoi_1"].sum(skipna=True)),
            "1_3": int(exp_stats["1_3"].sum(skipna=True)),
            "4_6": int(exp_stats["4_6"].sum(skipna=True)),
            "Tren_7": int(exp_stats["Tren_7"].sum(skipna=True)),
        }
        exp_stats = pd.concat([exp_stats, pd.DataFrame([summary_row])], ignore_index=True)

        # Ghi ra sheet Excel "nam_kinh_nghiem" (thay th·∫ø sheet n·∫øu ƒë√£ t·ªìn t·∫°i)
        try:
            with pd.ExcelWriter(out_phantich, engine="openpyxl", mode="a", if_sheet_exists="replace") as writer:
                exp_stats.to_excel(writer, sheet_name="nam_kinh_nghiem", index=False)
            print(f"üìÑ ƒê√£ l∆∞u {len(exp_stats)} d√≤ng (sheet: 'nam_kinh_nghiem').")
        except Exception as e:
            print(f"‚ùå L·ªói khi ghi Excel (sheet kinh nghi·ªám): {e}")

        # ===================================================
        # ==== 7) Th·ªëng k√™ ng√¥n ng·ªØ CV (ch·ªâ ghi sheet 'ngon_ngu_cv') ====
        LANG_COL = "ngon_ngu_cv"
        TARGET_COL = "nganh"  # ƒê√£ ƒë∆∞·ª£c s·ª≠ d·ª•ng ·ªü c√°c b∆∞·ªõc tr∆∞·ªõc (c·ªôt ng√†nh chu·∫©n)

        import unicodedata as _ud
        from collections import Counter

        def _no_accent(s: str) -> str:
            """
            Chu·∫©n ho√° chu·ªói v·ªÅ d·∫°ng kh√¥ng d·∫•u, lower-case, trim.
            Tr·∫£ v·ªÅ "" n·∫øu ƒë·∫ßu v√†o l√† None.
            """
            if s is None:
                return ""
            s = _ud.normalize("NFD", str(s))
            return "".join(ch for ch in s if _ud.category(ch) != "Mn").lower().strip()

        # T·ª´ ƒëi·ªÉn ng√¥n ng·ªØ ‚Üí c√°c alias (bao g·ªìm t√™n ng√¥n ng·ªØ & ch·ª©ng ch·ªâ li√™n quan)
        # L∆∞u √Ω: ph√°t hi·ªán theo "substring contains" sau khi normalize kh√¥ng d·∫•u
        _LANG_ALIASES = {
            "Ti·∫øng Anh": [
                "tieng anh", "anh", "english", "en",
                "ielts", "toefl", "toeic", "cambridge", "sat", "gre", "gmat"
            ],
            "Ti·∫øng Nh·∫≠t": [
                "tieng nhat", "nhat", "japanese", "nihongo", "jp", "nihon",
                "jlpt", "n1", "n2", "n3", "n4", "n5"
            ],
            "Ti·∫øng Trung": [
                "tieng trung", "trung", "chinese", "mandarin", "zhong", "zh",
                "putonghua", "han ngu", "hoa",
                "hsk", "hsk1", "hsk2", "hsk3", "hsk4", "hsk5", "hsk6"
            ],
            "Ti·∫øng H√†n": [
                "tieng han", "han", "korean", "hangul", "kr", "han quoc",
                "topik", "topik1", "topik2", "topik3", "topik4", "topik5", "topik6"
            ],
            "Ti·∫øng ƒê·ª©c": [
                "tieng duc", "duc", "german", "deutsch", "de",
                "goethe", "testdaf", "dsh", "telc"
            ],
            "Ti·∫øng Ph√°p": [
                "tieng phap", "phap", "french", "francais", "fr",
                "delf", "dalf", "tef", "tcf"
            ],
            "Ti·∫øng T√¢y Ban Nha": [
                "tieng tay ban nha", "tay ban nha", "spanish", "espanol", "es",
                "dele"
            ],
            "Ti·∫øng √ù": [
                "tieng y", "italian", "it", "celi", "cils", "plida"
            ],
            "Ti·∫øng Nga": [
                "tieng nga", "nga", "russian", "ru",
                "torfl"
            ],
            "Ti·∫øng Th√°i": [
                "tieng thai", "thai"
            ],
            "Ti·∫øng Vi·ªát": [
                "tieng viet", "viet", "vietnamese", "vi"
            ],
            "B·∫•t k·ª≥": [
                "bat ky", "batki", "bat-ky", "bat_ky"
            ],  # Kh√¥ng y√™u c·∫ßu ng√¥n ng·ªØ c·ª• th·ªÉ
        }

        # B·∫£n ƒë·ªì alias (ƒë√£ normalize) ‚Üí t√™n ng√¥n ng·ªØ chu·∫©n (canon)
        _ALIAS2CANON = {_no_accent(a): canon for canon, aliases in _LANG_ALIASES.items() for a in aliases}

        def _detect_langs(text: str) -> set[str]:
            """
            Ph√°t hi·ªán t·∫≠p ng√¥n ng·ªØ xu·∫•t hi·ªán trong chu·ªói:
            - Chu·∫©n ho√° chu·ªói v·ªÅ kh√¥ng d·∫•u/lower.
            - D√≤ theo "substring" v·ªõi c√°c alias.
            - Tr·∫£ v·ªÅ set t√™n ng√¥n ng·ªØ chu·∫©n (c√≥ th·ªÉ nhi·ªÅu h∆°n 1 n·∫øu text ƒë·ªÅ c·∫≠p nhi·ªÅu ng√¥n ng·ªØ).
            """
            s = _no_accent(text)
            if not s:
                return set()
            return {canon for alias_norm, canon in _ALIAS2CANON.items() if alias_norm and alias_norm in s}

        # N·∫øu thi·∫øu c·ªôt ngu·ªìn ng√¥n ng·ªØ ‚Üí b·ªè qua b∆∞·ªõc 7
        if LANG_COL not in df.columns:
            print(f"‚ö†Ô∏è B·ªè qua th·ªëng k√™ ng√¥n ng·ªØ (thi·∫øu c·ªôt '{LANG_COL}').")
        else:
            total_rows = len(df)  # S·ªë d√≤ng g·ªëc (l√†m m·∫´u s·ªë t√≠nh t·ª∑ l·ªá)
            total_counts = Counter()  # ƒê·∫øm s·ªë l·∫ßn ph√°t hi·ªán theo t·ª´ng ng√¥n ng·ªØ
            pairs_rows = []  # L∆∞u c·∫∑p (ng√¥n ng·ªØ, ng√†nh) ƒë·ªÉ l·∫•y top ng√†nh theo ng√¥n ng·ªØ

            # Chu·∫©n ho√° ng√†nh r·ªóng ‚Üí 'no_info' ƒë·ªÉ tr√°nh NaN khi groupby
            s_nganh_clean = (
                df[TARGET_COL]
                .astype(str).str.strip()
                .str.replace(r"^\s*$", "no_info", regex=True)
            )

            # Qu√©t t·ª´ng d√≤ng, ph√°t hi·ªán c√°c ng√¥n ng·ªØ ƒë∆∞·ª£c nh·∫Øc t·ªõi v√† gom c·∫∑p (ng√¥n ng·ªØ, ng√†nh)
            for idx, row in df.iterrows():
                langs = _detect_langs(row.get(LANG_COL, ""))
                if not langs:
                    continue
                nganh_val = s_nganh_clean.iat[idx]
                for lg in langs:
                    total_counts[lg] += 1
                    pairs_rows.append({"ngon_ngu": lg, "nganh": nganh_val})

            # Kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c ng√¥n ng·ªØ n√†o ‚Üí ghi sheet r·ªóng v·ªõi schema chu·∫©n
            if not total_counts:
                lang_df = pd.DataFrame(columns=["ngon_ngu", "so_tin", "ty_le(%)", "top_nganh", "so_tin_top_nganh"])
            else:
                # B·∫£ng t·ªïng: m·ªói ng√¥n ng·ªØ + s·ªë d√≤ng c√≥ nh·∫Øc ƒë·∫øn ng√¥n ng·ªØ ƒë√≥
                lang_df = (
                    pd.DataFrame([{"ngon_ngu": k, "so_tin": v} for k, v in total_counts.items()])
                    .sort_values("so_tin", ascending=False, ignore_index=True)
                )
                # T·ª∑ l·ªá tr√™n t·ªïng s·ªë d√≤ng input (kh√¥ng ph·∫£i tr√™n s·ªë d√≤ng ph√°t hi·ªán)
                lang_df["ty_le(%)"] = (lang_df["so_tin"] / max(total_rows, 1) * 100).round(2)

                # T√≠nh "top ng√†nh" cho t·ª´ng ng√¥n ng·ªØ (kh√¥ng ghi sheet chi ti·∫øt pairs)
                pairs_df = pd.DataFrame(pairs_rows)
                top_per_lang = (
                    pairs_df.groupby(["ngon_ngu", "nganh"]).size().reset_index(name="so_tin_top_nganh")
                    .sort_values(["ngon_ngu", "so_tin_top_nganh", "nganh"], ascending=[True, False, True])
                    .groupby("ngon_ngu", as_index=False).head(1)
                    .rename(columns={"nganh": "top_nganh"})
                )
                lang_df = lang_df.merge(top_per_lang, on="ngon_ngu", how="left")

                # D√≤ng t·ªïng quan: t·ªïng s·ªë l·∫ßn ph√°t hi·ªán v√† t·ª∑ l·ªá tr√™n t·ªïng m·∫´u
                total_detected = int(lang_df["so_tin"].sum())
                summary_row = {
                    "ngon_ngu": "t·ªïng quan",
                    "so_tin": total_detected,
                    "ty_le(%)": round(total_detected / max(total_rows, 1) * 100.0, 2),
                    "top_nganh": pd.NA,
                    "so_tin_top_nganh": pd.NA,
                }
                lang_df = pd.concat([lang_df, pd.DataFrame([summary_row])], ignore_index=True)

            # Ghi sheet 'ngon_ngu_cv' (thay th·∫ø sheet n·∫øu ƒë√£ t·ªìn t·∫°i; kh√¥ng t·∫°o c√°c sheet kh√°c)
            try:
                with pd.ExcelWriter(out_phantich, engine="openpyxl", mode="a", if_sheet_exists="replace") as writer:
                    lang_df.to_excel(writer, sheet_name="ngon_ngu_cv", index=False)
                print(f"üìÑ ƒê√£ l∆∞u {len(lang_df)} d√≤ng (sheet: 'ngon_ngu_cv').")
            except Exception as e:
                # B·∫Øt l·ªói ghi file Excel c·ªßa b∆∞·ªõc ng√¥n ng·ªØ
                print(f"‚ùå L·ªói khi ghi Excel (sheet ngon_ngu_cv): {e}")

        # ============================================================================
        # ==== 8) Th·ªëng k√™ tr√¨nh ƒë·ªô h·ªçc v·∫•n (kh√¥ng t√°ch alias) ====
        EDU_COL = "trinh_do_hoc_van"
        TARGET_COL = "nganh"  # ƒê√£ d√πng ·ªü tr√™n l√†m c·ªôt ng√†nh chu·∫©n ƒë·ªÉ groupby

        if EDU_COL not in df.columns:
            print(f"‚ö†Ô∏è B·ªè qua th·ªëng k√™ h·ªçc v·∫•n (thi·∫øu c·ªôt '{EDU_COL}').")
        else:
            total_rows = len(df)

            # Chu·∫©n ho√° nh·∫π: √©p str + trim, n·∫øu r·ªóng th√¨ g√°n 'no_info' ƒë·ªÉ tr√°nh NaN khi groupby/pivot
            s_edu = (
                df[EDU_COL]
                .astype(str).str.strip()
                .str.replace(r"^\s*$", "no_info", regex=True)
            )
            s_nganh = (
                df[TARGET_COL]
                .astype(str).str.strip()
                .str.replace(r"^\s*$", "no_info", regex=True)
            )

            # ƒê·∫øm s·ªë tin theo tr√¨nh ƒë·ªô h·ªçc v·∫•n (m·ª©c ƒë·ªô xu·∫•t hi·ªán t·ª´ng gi√° tr·ªã)
            edu_counts = s_edu.value_counts(dropna=False).rename_axis("trinh_do").reset_index(name="so_tin")
            edu_counts = edu_counts.sort_values("so_tin", ascending=False, ignore_index=True)

            # T·ª∑ l·ªá % tr√™n t·ªïng s·ªë d√≤ng ƒë·∫ßu v√†o
            edu_counts["ty_le(%)"] = (edu_counts["so_tin"] / max(total_rows, 1) * 100).round(2)

            # T√¨m "ng√†nh c√≥ nhi·ªÅu tin nh·∫•t" cho t·ª´ng tr√¨nh ƒë·ªô (kh√¥ng ghi sheet pairs chi ti·∫øt)
            pairs_df = (
                pd.DataFrame({"trinh_do": s_edu, "nganh": s_nganh})
                .groupby(["trinh_do", "nganh"]).size().reset_index(name="so_tin_top_nganh")
            )
            top_per_edu = (
                pairs_df.sort_values(["trinh_do", "so_tin_top_nganh", "nganh"], ascending=[True, False, True])
                .groupby("trinh_do", as_index=False).head(1)
                .rename(columns={"nganh": "top_nganh"})
            )

            edu_df = edu_counts.merge(top_per_edu, on="trinh_do", how="left")

            # D√≤ng T·ªîNG QUAN: t·ªïng s·ªë l·∫ßn xu·∫•t hi·ªán + t·ª∑ l·ªá tr√™n t·ªïng m·∫´u (kh√¥ng c√≥ top_nganh)
            total_detected = int(edu_df["so_tin"].sum())
            summary_row = {
                "trinh_do": "T·ªîNG QUAN",
                "so_tin": total_detected,
                "ty_le(%)": round(total_detected / max(total_rows, 1) * 100.0, 2),
                "top_nganh": pd.NA,
                "so_tin_top_nganh": pd.NA,
            }
            edu_df = pd.concat([edu_df, pd.DataFrame([summary_row])], ignore_index=True)

            # Ghi ra sheet Excel 'trinh_do_hoc_van' (replace sheet n·∫øu ƒë√£ t·ªìn t·∫°i)
            try:
                with pd.ExcelWriter(out_phantich, engine="openpyxl", mode="a", if_sheet_exists="replace") as writer:
                    edu_df.to_excel(writer, sheet_name="trinh_do_hoc_van", index=False)
                print(f"üìÑ ƒê√£ l∆∞u {len(edu_df)} d√≤ng (sheet: 'trinh_do_hoc_van').")
            except Exception as e:
                print(f"‚ùå L·ªói khi ghi Excel (sheet trinh_do_hoc_van): {e}")
        # ===================================================================================
        # ==== 9) Th·ªëng k√™ lo·∫°i h√¨nh l√†m vi·ªác (sheet: 'loai_hinh_lam_viec') ====
        WORK_COL = "loai_hinh_lam_viec"
        TARGET_COL = "nganh"

        if WORK_COL not in df.columns:
            print(f"‚ö†Ô∏è B·ªè qua th·ªëng k√™ lo·∫°i h√¨nh l√†m vi·ªác (thi·∫øu c·ªôt '{WORK_COL}').")
        else:
            total_rows = len(df)

            # Chu·∫©n ho√° nh·∫π: √©p str + trim, r·ªóng -> 'no_info'
            s_work = (
                df[WORK_COL]
                .astype(str).str.strip()
                .str.replace(r"^\s*$", "no_info", regex=True)
            )
            s_nganh = (
                df[TARGET_COL]
                .astype(str).str.strip()
                .str.replace(r"^\s*$", "no_info", regex=True)
            )

            # ƒê·∫øm s·ªë tin theo lo·∫°i h√¨nh l√†m vi·ªác
            work_counts = s_work.value_counts(dropna=False).rename_axis("loai_hinh").reset_index(name="so_tin")
            work_counts = work_counts.sort_values("so_tin", ascending=False, ignore_index=True)

            # T·ª∑ l·ªá % tr√™n t·ªïng s·ªë d√≤ng
            work_counts["ty_le(%)"] = (work_counts["so_tin"] / max(total_rows, 1) * 100).round(2)

            # T√¨m ng√†nh ph·ªï bi·∫øn nh·∫•t cho t·ª´ng lo·∫°i h√¨nh (kh√¥ng ghi sheet pairs)
            pairs_df = (
                pd.DataFrame({"loai_hinh": s_work, "nganh": s_nganh})
                .groupby(["loai_hinh", "nganh"]).size().reset_index(name="so_tin_top_nganh")
            )
            top_per_work = (
                pairs_df.sort_values(["loai_hinh", "so_tin_top_nganh", "nganh"], ascending=[True, False, True])
                .groupby("loai_hinh", as_index=False).head(1)
                .rename(columns={"nganh": "top_nganh"})
            )

            work_df = work_counts.merge(top_per_work, on="loai_hinh", how="left")

            # D√≤ng T·ªîNG QUAN cho lo·∫°i h√¨nh l√†m vi·ªác
            total_detected = int(work_df["so_tin"].sum())
            summary_row = {
                "loai_hinh": "T·ªîNG QUAN",
                "so_tin": total_detected,
                "ty_le(%)": round(total_detected / max(total_rows, 1) * 100.0, 2),
                "top_nganh": pd.NA,
                "so_tin_top_nganh": pd.NA,
            }
            work_df = pd.concat([work_df, pd.DataFrame([summary_row])], ignore_index=True)

            # Ghi ra sheet 'loai_hinh_lam_viec' (replace sheet n·∫øu ƒë√£ t·ªìn t·∫°i)
            try:
                with pd.ExcelWriter(out_phantich, engine="openpyxl", mode="a", if_sheet_exists="replace") as writer:
                    work_df.to_excel(writer, sheet_name="loai_hinh_lam_viec", index=False)
                print(f"üìÑ ƒê√£ l∆∞u {len(work_df)} d√≤ng (sheet: 'loai_hinh_lam_viec').")
            except Exception as e:
                print(f"‚ùå L·ªói khi ghi Excel (sheet loai_hinh_lam_viec): {e}")

        # =============================================================================
        # ==== 10) Th·ªëng k√™ ƒë·ªô tu·ªïi theo ng√†nh & nh√≥m tu·ªïi ====
        TARGET_COL = "nganh"  # T√™n c·ªôt ng√†nh ƒë·ªÉ group c√°c th·ªëng k√™ theo ng√†nh

        def _no_accent(s: str) -> str:
            # Chu·∫©n ho√° kh√¥ng d·∫•u, lower, trim; None -> ""
            if s is None:
                return ""
            s = unicodedata.normalize("NFD", str(s))
            return "".join(ch for ch in s if unicodedata.category(ch) != "Mn").lower().strip()

        def _to_num(x):
            # √âp ki·ªÉu s·ªë an to√†n (l·ªói -> NaN)
            try:
                return pd.to_numeric(x, errors="coerce")
            except Exception:
                return np.nan

        def _extract_ages_from_text(s: str):
            """
            Tr√≠ch c√°c s·ªë (c√≥ th·ªÉ c√≥ d·∫•u ph·∫©y l√†m th·∫≠p ph√¢n, ngh√¨n) t·ª´ chu·ªói 'ƒë·ªô tu·ªïi'.
            V√≠ d·ª•: '18-24', 'Tr√™n 35', '1.000' ‚Üí chu·∫©n ho√° r·ªìi l·∫•y c√°c s·ªë float.
            """
            if not s:
                return []
            s1 = str(s)
            s1 = s1.replace(",", ".")
            s1 = re.sub(r"(?<=\d)\.(?=\d{3}\b)", "", s1)  # V√≠ d·ª• '1.000' -> '1000'
            nums = re.findall(r"\d+(?:\.\d+)?", s1)
            try:
                vals = [float(n) for n in nums]
            except Exception:
                vals = []
            return vals

        def _representative_age(row):
            """
            Tu·ªïi ƒë·∫°i di·ªán (d√πng ƒë·ªÉ bucket ho√°):
            - ∆Øu ti√™n med_tuoi (n·∫øu c√≥).
            - N·∫øu c√≥ min/max th√¨ l·∫•y trung b√¨nh (min+max)/2.
            - N·∫øu kh√¥ng, parse t·ª´ 'do_tuoi' t·ª± do b·∫±ng regex s·ªë.
            - Kh√¥ng suy ƒë∆∞·ª£c ‚Üí NaN.
            """
            minv = _to_num(row.get("min_tuoi")) if "min_tuoi" in row else np.nan
            maxv = _to_num(row.get("max_tuoi")) if "max_tuoi" in row else np.nan
            medv = _to_num(row.get("med_tuoi")) if "med_tuoi" in row else np.nan
            if pd.notna(medv):
                return float(medv)
            if pd.notna(minv) or pd.notna(maxv):
                a = minv if pd.notna(minv) else maxv
                b = maxv if pd.notna(maxv) else minv
                if pd.notna(a) and pd.notna(b):
                    return float((a + b) / 2.0)
                return float(a) if pd.notna(a) else float("nan")
            text = str(row.get("do_tuoi")) if "do_tuoi" in row else ""
            vals = _extract_ages_from_text(text)
            if vals:
                return float(np.mean(vals))
            return np.nan

        def _is_no_info_age(row):
            """
            C·ªù 'kh√¥ng y√™u c·∫ßu/kh√¥ng gi·ªõi h·∫°n tu·ªïi' ho·∫∑c kh√¥ng suy ƒë∆∞·ª£c tu·ªïi ƒë·∫°i di·ªán.
            D√πng ƒë·ªÉ ƒë·∫©y v√†o bucket 'no_info'.
            """
            text = _no_accent(row.get("do_tuoi", ""))
            if "khong yeu cau" in text or "khong gioi han" in text or "khong bat buoc" in text:
                return True
            rep = row.get("_age_rep")
            return not (pd.notna(rep) and np.isfinite(rep))

        def _bucket_age(val):
            # Ph√¢n bucket tu·ªïi theo kho·∫£ng: 15‚Äì24, 25‚Äì34, 35‚Äì54, 55+, ho·∫∑c None (n·∫øu kh√¥ng x·∫øp ƒë∆∞·ª£c)
            if pd.isna(val) or not np.isfinite(val):
                return None
            if 15 <= val <= 24:
                return "15‚Äì24"
            if 24 < val <= 34:
                return "25‚Äì34"
            if 34 < val <= 54:
                return "35‚Äì54"
            if val > 54:
                return "55+"
            return None

        if TARGET_COL not in df.columns:
            print(f"‚ö†Ô∏è B·ªè qua th·ªëng k√™ ƒë·ªô tu·ªïi: thi·∫øu c·ªôt '{TARGET_COL}'.")
        else:
            # 1) Chu·∫©n ho√° ng√†nh (r·ªóng -> 'no_info')
            s_nganh = (
                df[TARGET_COL].astype(str).str.strip()
                .replace(r"^\s*$", "no_info", regex=True)
            )

            # 2) T√≠nh tu·ªïi ƒë·∫°i di·ªán cho t·ª´ng d√≤ng (ph·ª•c v·ª• ph√¢n bucket)
            df["_age_rep"] = df.apply(_representative_age, axis=1)

            # √âp ki·ªÉu s·ªë ngu·ªìn (n·∫øu c√≥ c·ªôt) ƒë·ªÉ t√≠nh min/max/mean theo ng√†nh
            if "min_tuoi" in df.columns:
                df["_min_src"] = pd.to_numeric(df["min_tuoi"], errors="coerce")
            else:
                df["_min_src"] = np.nan
            if "max_tuoi" in df.columns:
                df["_max_src"] = pd.to_numeric(df["max_tuoi"], errors="coerce")
            else:
                df["_max_src"] = np.nan
            if "med_tuoi" in df.columns:
                df["_med_src"] = pd.to_numeric(df["med_tuoi"], errors="coerce")
            else:
                df["_med_src"] = np.nan

            # Th·ªëng k√™ theo ng√†nh: min/max/mean (mean d·ª±a tr√™n med_tuoi n·∫øu c√≥)
            age_stats = (
                pd.DataFrame({
                    TARGET_COL: s_nganh,
                    "_min_src": df["_min_src"],
                    "_max_src": df["_max_src"],
                    "_med_src": df["_med_src"],
                })
                .groupby(TARGET_COL, dropna=False)
                .agg(
                    min_tuoi=("_min_src", "min"),
                    max_tuoi=("_max_src", "max"),
                    mean_tuoi=("_med_src", "mean"),
                )
                .reset_index()
            )
            if not age_stats.empty:
                age_stats["mean_tuoi"] = age_stats["mean_tuoi"].round(2)

            # 4) ƒê·∫øm s·ªë tin theo nh√≥m tu·ªïi (bao g·ªìm 'no_info')
            df["_no_info_age"] = df.apply(_is_no_info_age, axis=1)
            df["_age_bucket"] = df["_age_rep"].apply(_bucket_age)
            bucket_series = np.where(df["_no_info_age"], "no_info", df["_age_bucket"].astype(object))
            bucket_series = pd.Series(bucket_series).fillna("no_info")

            buck_tbl = pd.DataFrame({
                TARGET_COL: s_nganh,
                "age_bucket": bucket_series
            })

            bucket_order = ["15‚Äì24", "25‚Äì34", "35‚Äì54", "55+", "no_info"]

            # B·∫£ng ƒë·∫øm d√†i ‚Üí pivot r·ªông theo bucket
            pivot_counts = (
                buck_tbl.groupby([TARGET_COL, "age_bucket"], dropna=False)
                .size()
                .rename("so_tin")
                .reset_index()
            )

            counts_wide = (
                pivot_counts.pivot(index=TARGET_COL, columns="age_bucket", values="so_tin")
                .reindex(columns=bucket_order)
                .fillna(0)
                .astype(int)
                .reset_index()
            )
            counts_wide["tong_so_tin"] = counts_wide[bucket_order].sum(axis=1)

            # 5) G·ªôp th·ªëng k√™ min/max/mean + b·∫£ng ƒë·∫øm bucket ‚Üí do_tuoi_df
            do_tuoi_df = (
                counts_wide.merge(age_stats, on=TARGET_COL, how="left")
                .loc[:, [TARGET_COL, "min_tuoi", "max_tuoi", "mean_tuoi"] + bucket_order + ["tong_so_tin"]]
            )

            # 6) D√≤ng T·ªîNG: t√≠nh t·ª´ ƒë√∫ng c·ªôt ngu·ªìn (to√†n b·ªô file)
            total_row = {
                TARGET_COL: "T·ªîNG",
                "min_tuoi": (pd.to_numeric(df["_min_src"], errors="coerce").dropna().min()
                             if "_min_src" in df.columns else np.nan),
                "max_tuoi": (pd.to_numeric(df["_max_src"], errors="coerce").dropna().max()
                             if "_max_src" in df.columns else np.nan),
                "mean_tuoi": (round(float(pd.to_numeric(df["_med_src"], errors="coerce").dropna().mean()), 2)
                              if "_med_src" in df.columns and pd.to_numeric(df["_med_src"],
                                                                            errors="coerce").notna().any()
                              else np.nan),
            }
            for b in bucket_order + ["tong_so_tin"]:
                total_row[b] = int(do_tuoi_df[b].sum()) if b in do_tuoi_df.columns else 0

            # 7) D√≤ng TOP NG√ÄNH: ng√†nh c√≥ s·ªë tin l·ªõn nh·∫•t theo t·ª´ng bucket
            top_row = {TARGET_COL: "TOP NG√ÄNH", "min_tuoi": pd.NA, "max_tuoi": pd.NA, "mean_tuoi": pd.NA}
            for b in bucket_order:
                if b in do_tuoi_df.columns and not do_tuoi_df.empty:
                    idx = do_tuoi_df[b].astype(int).idxmax()
                    if pd.notna(idx):
                        top_ind = do_tuoi_df.loc[idx, TARGET_COL]
                        top_val = int(do_tuoi_df.loc[idx, b])
                        top_row[b] = f"{top_ind} ({top_val})"
                    else:
                        top_row[b] = pd.NA
                else:
                    top_row[b] = pd.NA
            top_row["tong_so_tin"] = pd.NA

            # 8) Th√™m 2 d√≤ng t·ªïng & top v√†o cu·ªëi, ghi sheet 'do_tuoi'
            add_rows = pd.DataFrame([total_row, top_row]).dropna(axis=1, how="all")
            do_tuoi_out = pd.concat([do_tuoi_df, add_rows], ignore_index=True)

            try:
                with pd.ExcelWriter(out_phantich, engine="openpyxl", mode="a", if_sheet_exists="replace") as writer:
                    do_tuoi_out.to_excel(writer, sheet_name="do_tuoi", index=False)
                print(f"üìÑ ƒê√£ l∆∞u {len(do_tuoi_out)} d√≤ng (sheet: 'do_tuoi').")
            except Exception as e:
                # Ghi log n·∫øu l·ªói khi ghi sheet ƒë·ªô tu·ªïi
                print(f"‚ùå L·ªói khi ghi Excel (do_tuoi): {e}")

    #===========================================================================
        # ==== 11) Ph√¢n t√≠ch Ng√†y l√†m vi·ªác theo ng√†nh (sheet: ngay_lam_viec) ====
        NGAY_COL = "ngay_lam_viec"  # S·ª≠a l·ªói ch√≠nh t·∫£ (t√™n c·ªôt chu·∫©n)
        SONGAY_COL = "so_ngay_lam"

        # Alias chu·∫©n ho√° ƒë·ªÉ tr√°nh l·ªói scope/closure:
        # ∆Øu ti√™n d√πng _norm_text_no_accent; n·∫øu kh√¥ng c√≥ th√¨ fallback sang _no_accent; cu·ªëi c√πng t·ª± ƒë·ªãnh nghƒ©a.
        try:
            _NORM = _norm_text_no_accent  # ƒê√£ c√≥ s·∫µn ·ªü c√°c ph·∫ßn tr∆∞·ªõc
        except NameError:
            try:
                _NORM = _no_accent  # Fallback n·∫øu d·ª± √°n d√πng t√™n n√†y
            except NameError:
                import unicodedata as _ud
                def _NORM(s: str) -> str:  # Fallback cu·ªëi c√πng: normalize kh√¥ng d·∫•u + lower + trim
                    if s is None:
                        return ""
                    t = _ud.normalize("NFD", str(s))
                    t = "".join(ch for ch in t if _ud.category(ch) != "Mn")
                    return t.lower().strip()

        def _to_int_safe(x):
            # √âp ki·ªÉu s·ªë nguy√™n an to√†n; l·ªói/NaN -> None
            try:
                v = pd.to_numeric(x, errors="coerce")
                if pd.isna(v):
                    return None
                return int(round(float(v)))
            except Exception:
                return None

        def _is_no_info_text(t: str) -> bool:
            # X√°c ƒë·ªãnh chu·ªói "kh√¥ng th√¥ng tin/kh√¥ng y√™u c·∫ßu" theo t·∫≠p c√°c c·ª•m t·ª´ ph·ªï bi·∫øn
            if not t:
                return True
            if t in {"", "na", "no_info"}:
                return True
            return any(pat in t for pat in [
                "khong yeu cau", "khong bat buoc", "khong ro", "khong xac dinh",
                "khong de cap", "khong quy dinh", "bo qua", "khong co thong tin"
            ])

        def _classify_ngay(text_raw: str, so_ngay):
            """
            Ph√¢n lo·∫°i nh√≥m ng√†y l√†m vi·ªác:
            - ∆Øu ti√™n rule d·ª±a tr√™n text (T2‚ÄìT6 / T2‚ÄìT7) sau khi chu·∫©n ho√°.
            - N·∫øu text m∆° h·ªì ‚Üí d√πng so_ngay_lam (5 ‚Üí T2‚ÄìT6; 6/7 ‚Üí T2‚ÄìT7).
            - N·∫øu v·∫´n kh√¥ng x√°c ƒë·ªãnh ‚Üí 'no_info'; c√≤n l·∫°i ‚Üí 'Khac'.
            """
            t = _NORM(text_raw)
            t_compact = t.replace(" ", "").replace("thu", "t")  # "th·ª© 2" ‚Üí "t2"

            # 0) Tr∆∞·ªùng h·ª£p kh√¥ng c√≥ th√¥ng tin
            if _is_no_info_text(t) and (so_ngay is None):
                return "no_info"

            # 1) Nh·∫≠n d·∫°ng tr·ª±c ti·∫øp qua text
            if ("t2" in t_compact and "t6" in t_compact) and ("t7" not in t_compact):
                return "T2-T6"
            if "t2" in t_compact and "t7" in t_compact:
                return "T2-T7"

            # Alias th∆∞·ªùng g·∫∑p cho t·ª´ng nh√≥m
            alias_t2t6 = [
                "thu 2 - thu 6", "thu hai den thu sau", "tu thu 2 den thu 6",
                "t2-t6", "t2 den t6", "lam hanh chinh", "hanh chinh"
            ]
            alias_t2t7 = [
                "thu 2 - thu 7", "thu hai den thu bay", "tu thu 2 den thu 7",
                "t2-t7", "t2 den t7", "6 ngay/tuan", "7 ngay/tuan", "lam ca thu 7"
            ]
            if any(a.replace(" ", "") in t_compact for a in alias_t2t6):
                return "T2-T6"
            if any(a.replace(" ", "") in t_compact for a in alias_t2t7):
                return "T2-T7"

            # 2) S·ª≠ d·ª•ng s·ªë ng√†y n·∫øu text kh√¥ng r√µ
            if so_ngay is not None:
                if so_ngay == 5:
                    return "T2-T6"
                if so_ngay in (6, 7):
                    return "T2-T7"

            # 3) Kh√¥ng r√µ r√†ng ‚Üí 'no_info'
            if _is_no_info_text(t) or (so_ngay is None and not t):
                return "no_info"

            # 4) C√°c tr∆∞·ªùng h·ª£p c√≤n l·∫°i g√°n 'Khac'
            return "Khac"

        # Ki·ªÉm tra c·ªôt ngu·ªìn b·∫Øt bu·ªôc
        if TARGET_COL not in df.columns:
            print(f"‚ö†Ô∏è B·ªè qua ph√¢n t√≠ch 'ngay_lam_viec': thi·∫øu c·ªôt '{TARGET_COL}'.")
        elif NGAY_COL not in df.columns and SONGAY_COL not in df.columns:
            print(f"‚ö†Ô∏è B·ªè qua ph√¢n t√≠ch 'ngay_lam_viec': thi·∫øu c·∫£ '{NGAY_COL}' v√† '{SONGAY_COL}'.")
        else:
            # 1) Chu·∫©n ho√° ng√†nh (r·ªóng ‚Üí 'no_info')
            s_nganh = (
                df[TARGET_COL].astype(str).str.strip()
                .replace(r"^\s*$", "no_info", regex=True)
            )

            # 2) L·∫•y 2 c·ªôt ngu·ªìn (broadcast ƒë√∫ng index, ƒëi·ªÅn m·∫∑c ƒë·ªãnh khi thi·∫øu)
            s_ngay_raw = df.get(NGAY_COL, pd.Series("", index=df.index)).astype(str).reindex(df.index, fill_value="")
            s_so_ngay = df.get(SONGAY_COL, pd.Series(np.nan, index=df.index)).reindex(df.index).map(_to_int_safe)

            # 3) Ph√¢n lo·∫°i nh√≥m ng√†y + l∆∞u text ƒë√£ chu·∫©n ho√° (ph·ª•c v·ª• "Chi_tiet_khac")
            nhom_vals, ngay_raw_norm_vals = [], []
            for txt, sn in zip(s_ngay_raw, s_so_ngay):
                nhom = _classify_ngay(txt, sn)
                nhom_vals.append(nhom)
                ngay_raw_norm_vals.append(_NORM(txt) or ("so_ngay=" + (str(sn) if sn is not None else "na")))

            df["_ngay_nhom"] = pd.Series(nhom_vals, index=df.index)
            df["_ngay_raw_norm"] = pd.Series(ngay_raw_norm_vals, index=df.index)

            # 4) ƒê·∫øm theo ng√†nh x nh√≥m (bao g·ªìm 'no_info')
            bucket_order = ["T2-T6", "T2-T7", "Khac", "no_info"]

            grp = (
                pd.DataFrame({TARGET_COL: s_nganh, "_ngay_nhom": df["_ngay_nhom"]})
                .groupby([TARGET_COL, "_ngay_nhom"], dropna=False)
                .size()
                .rename("so_tin")
                .reset_index()
            )

            counts_wide = (
                grp.pivot(index=TARGET_COL, columns="_ngay_nhom", values="so_tin")
                .reindex(columns=bucket_order)
                .fillna(0)
                .astype(int)
                .reset_index()
            )

            # 5) Chi ti·∫øt 'Khac': g·ªôp gi√° tr·ªã raw chu·∫©n ho√° k√®m danh s√°ch link (n·∫øu c√≥)
            #    Output: "gia_tri_khac | url1; url2, gia_tri_khac_2 | url3"
            def _pick_href_col(_df):
                # T·ª± ƒë·ªông ch·ªçn c·ªôt link ph√π h·ª£p n·∫øu t·ªìn t·∫°i
                for c in ["href", "link", "url", "job_url", "job_href", "source_url"]:
                    if c in _df.columns:
                        return c
                return None

            href_col = _pick_href_col(df)

            d_khac = pd.DataFrame({
                TARGET_COL: s_nganh,
                "_ngay_nhom": df["_ngay_nhom"],
                "_ngay_raw_norm": df["_ngay_raw_norm"],
                "_href": (df[href_col].astype(str) if href_col else pd.Series("", index=df.index))
            })

            d_khac = d_khac.loc[d_khac["_ngay_nhom"] == "Khac"].copy()
            d_khac["_href"] = d_khac["_href"].fillna("").str.strip()
            d_khac["_ngay_raw_norm"] = d_khac["_ngay_raw_norm"].fillna("").str.strip()
            d_khac = d_khac.loc[
                (d_khac["_ngay_raw_norm"] != "") &
                (~d_khac["_ngay_raw_norm"].isin({"na", "no_info"}))
                ]

            if not d_khac.empty:
                # Gom theo (ng√†nh, gi√° tr·ªã kh√°c) ‚Üí g·ªôp link (unique) b·∫±ng "; "
                agg = (
                    d_khac.groupby([TARGET_COL, "_ngay_raw_norm"])["_href"]
                    .apply(lambda s: "; ".join(sorted(set([x for x in s if x]))))
                    .reset_index()
                )
                agg["pair"] = agg.apply(
                    lambda r: f"{r['_ngay_raw_norm']} | {r['_href']}" if r[
                        "_href"] else f"{r['_ngay_raw_norm']} | (no_link)",
                    axis=1
                )
                khac_detail = (
                    agg.groupby(TARGET_COL)["pair"]
                    .apply(lambda s: ", ".join(s))
                    .rename("Chi_tiet_khac")
                    .reset_index()
                )
            else:
                # Kh√¥ng c√≥ b·∫£n ghi 'Khac' ‚Üí b·∫£ng chi ti·∫øt r·ªóng
                khac_detail = pd.DataFrame({TARGET_COL: [], "Chi_tiet_khac": []})

            # === G·ªôp th√†nh b·∫£ng n·ªÅn k·∫øt qu·∫£ (counts_wide + Chi_tiet_khac) ===
            kq = counts_wide.merge(khac_detail, on=TARGET_COL, how="left")
            kq["Chi_tiet_khac"] = kq["Chi_tiet_khac"].fillna("")
            kq["tong_so_tin"] = kq[bucket_order].sum(axis=1)

            # 6) D√≤ng T·ªîNG: t·ªïng s·ªë tin theo t·ª´ng bucket v√† t·ªïng to√†n b·∫£ng
            total_row = {
                TARGET_COL: "T·ªîNG",
                **{b: int(kq[b].sum()) if b in kq.columns else 0 for b in bucket_order},
                "Chi_tiet_khac": "",
                "tong_so_tin": int(kq["tong_so_tin"].sum()) if "tong_so_tin" in kq.columns else 0,
            }

            # 7) D√≤ng TOP NG√ÄNH: ng√†nh c√≥ s·ªë tin l·ªõn nh·∫•t ·ªü m·ªói bucket
            top_row = {TARGET_COL: "TOP NG√ÄNH", "Chi_tiet_khac": "", "tong_so_tin": pd.NA}
            if not kq.empty:
                for b in bucket_order:
                    if b in kq.columns and kq[b].notna().any():
                        idx = kq[b].astype(int).idxmax()
                        top_ind = kq.loc[idx, TARGET_COL]
                        top_val = int(kq.loc[idx, b])
                        top_row[b] = f"{top_ind} ({top_val})"
                    else:
                        top_row[b] = pd.NA
            else:
                for b in bucket_order:
                    top_row[b] = pd.NA

            add_rows = pd.DataFrame([total_row, top_row]).dropna(axis=1, how="all")
            ngaylv_out = pd.concat([kq, add_rows], ignore_index=True)

            # 8) Ghi Excel: sheet 'ngay_lam_viec' (replace sheet n·∫øu ƒë√£ t·ªìn t·∫°i)
            try:
                with pd.ExcelWriter(out_phantich, engine="openpyxl", mode="a", if_sheet_exists="replace") as writer:
                    ngaylv_out.to_excel(writer, sheet_name="ngay_lam_viec", index=False)
                print(f"üìÑ ƒê√£ l∆∞u {len(ngaylv_out)} d√≤ng (sheet: 'ngay_lam_viec').")
            except Exception as e:
                # Log l·ªói khi ghi file
                print(f"‚ùå L·ªói khi ghi Excel (ngay_lam_viec): {e}")
        # ===========================================================================
        # ==== 12 Th·ªëng k√™ gi·ªù l√†m vi·ªác theo ng√†nh ====
        TARGET_COL = "nganh"  # T√™n c·ªôt ng√†nh ƒë·ªÉ groupby
        COL_START = "gio_bat_dau"  # Gi·ªù b·∫Øt ƒë·∫ßu (h·ªó tr·ª£ nhi·ªÅu ƒë·ªãnh d·∫°ng: 8, 08:00, 8h30, 0830, 8.30, ...)
        COL_END = "gio_ket_thuc"  # Gi·ªù k·∫øt th√∫c (c√°c ƒë·ªãnh d·∫°ng t∆∞∆°ng t·ª± COL_START)
        COL_HOURS = "so_gio_lam_ngay"  # S·ªë gi·ªù l√†m/ng√†y (gi√° tr·ªã s·ªë)

        def _safe_str(x) -> str:
            return "" if pd.isna(x) else str(x).strip()

        def _parse_time_to_minutes(x):
            # Chu·∫©n ho√° chu·ªói gi·ªù v·ªÅ ph√∫t k·ªÉ t·ª´ 00:00 (None n·∫øu kh√¥ng parse ƒë∆∞·ª£c)
            s = _safe_str(x).lower().replace(".", ":")
            if not s:
                return None

            # Quy v·ªÅ d·∫°ng chung: '8h30' -> '8:30', '8h' -> '8:00'; lo·∫°i b·ªè k√Ω t·ª± l·∫° ngo√†i [0-9:]
            s = re.sub(r"h", ":", s)
            s = re.sub(r"[^\d:]", "", s)  # ch·ªâ gi·ªØ ch·ªØ s·ªë v√† ':'

            # Tr∆∞·ªùng h·ª£p to√†n s·ªë (kh√¥ng c√≥ ':'): '8', '0830', '800'
            if s.isdigit():
                try:
                    if len(s) <= 2:
                        hh = int(s)
                        if 0 <= hh <= 24:
                            return hh * 60
                    elif len(s) == 3:
                        hh = int(s[0])
                        mm = int(s[1:])
                        if 0 <= hh <= 24 and 0 <= mm < 60:
                            return hh * 60 + mm
                    elif len(s) == 4:
                        hh = int(s[:2]);
                        mm = int(s[2:])
                        if 0 <= hh <= 24 and 0 <= mm < 60:
                            return hh * 60 + mm
                except:
                    return None
                return None

            # C√°c d·∫°ng c√≥ ':' (v√≠ d·ª• '8:30', '08:00', '8:')
            try:
                parts = s.split(":")
                if len(parts) == 1:
                    hh = int(parts[0]);
                    mm = 0
                elif len(parts) >= 2:
                    hh = int(parts[0]) if parts[0] else 0
                    mm = int(parts[1]) if parts[1] else 0
                else:
                    return None

                if hh == 24 and mm == 0:
                    return 24 * 60  # cho ph√©p '24:00'
                if 0 <= hh <= 23 and 0 <= mm < 60:
                    return hh * 60 + mm
            except:
                return None
            return None

        def _fmt_minutes(m):
            # ƒê·ªãnh d·∫°ng ph√∫t -> 'HH:MM' (gi·ªõi h·∫°n 00:00..24:00; None/NaN -> "")
            if m is None or (isinstance(m, float) and math.isnan(m)):
                return ""
            m = int(m)
            if m < 0: return ""
            if m > 24 * 60: m = 24 * 60
            hh = m // 60;
            mm = m % 60
            return f"{hh:02d}:{mm:02d}"

        def _to_float(x):
            # √âp ki·ªÉu s·ªë float an to√†n (Kh√¥ng h·ª£p l·ªá -> None)
            try:
                v = pd.to_numeric(x, errors="coerce")
                return float(v) if not pd.isna(v) else None
            except:
                return None

        # Ki·ªÉm tra c√°c c·ªôt b·∫Øt bu·ªôc; n·∫øu thi·∫øu -> b·ªè qua to√†n b·ªô th·ªëng k√™ gi·ªù l√†m
        if TARGET_COL not in df.columns or COL_START not in df.columns or COL_END not in df.columns or COL_HOURS not in df.columns:
            print(
                f"‚ö†Ô∏è B·ªè qua th·ªëng k√™ gi·ªù l√†m: thi·∫øu c·ªôt b·∫Øt bu·ªôc. C·∫ßn c√≥: '{TARGET_COL}', '{COL_START}', '{COL_END}', '{COL_HOURS}'.")
        else:
            d = df.copy()

            # Chu·∫©n ho√° ng√†nh (r·ªóng -> 'no_info' ƒë·ªÉ tr√°nh NaN khi group/pivot)
            s_nganh = (
                d[TARGET_COL].astype(str).str.strip()
                .replace(r"^\s*$", "no_info", regex=True)
            )

            # Parse gi·ªù b·∫Øt ƒë·∫ßu/k·∫øt th√∫c -> ph√∫t; s·ªë gi·ªù l√†m -> float
            d["_start_min"] = d[COL_START].map(_parse_time_to_minutes)
            d["_end_min"] = d[COL_END].map(_parse_time_to_minutes)
            d["_hours"] = d[COL_HOURS].map(_to_float)

            # H√†m agg an to√†n (b·ªè None/NaN tr∆∞·ªõc khi t√≠nh)
            def _min_ignore_null(series):
                s = pd.to_numeric(pd.Series([v for v in series if v is not None]), errors="coerce").dropna()
                return s.min() if len(s) else None

            def _max_ignore_null(series):
                s = pd.to_numeric(pd.Series([v for v in series if v is not None]), errors="coerce").dropna()
                return s.max() if len(s) else None

            def _mean_ignore_null(series):
                s = pd.to_numeric(pd.Series([v for v in series if v is not None]), errors="coerce").dropna()
                return float(s.mean()) if len(s) else None

            # 1) Th·ªëng k√™ theo ng√†nh: gi·ªù b·∫Øt ƒë·∫ßu/k·∫øt th√∫c s·ªõm-mu·ªôn nh·∫•t, TB s·ªë gi·ªù/ng√†y, s·ªë tin
            glv_stats = (
                pd.DataFrame(
                    {TARGET_COL: s_nganh, "_start_min": d["_start_min"], "_end_min": d["_end_min"],
                     "_hours": d["_hours"]})
                .groupby(TARGET_COL, dropna=False)
                .agg(
                    gio_bat_dau_som_nhat=("_start_min", _min_ignore_null),
                    gio_bat_dau_muon_nhat=("_start_min", _max_ignore_null),
                    gio_ket_thuc_som_nhat=("_end_min", _min_ignore_null),
                    gio_ket_thuc_muon_nhat=("_end_min", _max_ignore_null),
                    tb_so_gio_lam_ngay=("_hours", _mean_ignore_null),
                    so_tin=("._start_min".replace(".", ""), "size"),
                    # ƒê·∫øm b·∫£n ghi theo group (k·ªπ thu·∫≠t ƒë·∫∑t t√™n ƒë·ªÉ l·∫•y size)
                )
                .reset_index()
            )

            # 2) ƒê·ªãnh d·∫°ng th·ªùi gian (HH:MM) & l√†m tr√≤n s·ªë gi·ªù
            for c in ["gio_bat_dau_som_nhat", "gio_bat_dau_muon_nhat", "gio_ket_thuc_som_nhat",
                      "gio_ket_thuc_muon_nhat"]:
                glv_stats[c] = glv_stats[c].map(_fmt_minutes)
            glv_stats["tb_so_gio_lam_ngay"] = glv_stats["tb_so_gio_lam_ngay"].map(
                lambda x: None if x is None else round(float(x), 2))

            # 3) D√≤ng T·ªîNG QUAN (to√†n b·ªô dataset)
            overall_start_min_min = _min_ignore_null(d["_start_min"])
            overall_start_min_max = _max_ignore_null(d["_start_min"])
            overall_end_min_min = _min_ignore_null(d["_end_min"])
            overall_end_min_max = _max_ignore_null(d["_end_min"])
            overall_hours_mean = _mean_ignore_null(d["_hours"])
            overall_count = int(len(d))

            total_row = {
                TARGET_COL: "T·ªîNG QUAN",
                "gio_bat_dau_som_nhat": _fmt_minutes(overall_start_min_min),
                "gio_bat_dau_muon_nhat": _fmt_minutes(overall_start_min_max),
                "gio_ket_thuc_som_nhat": _fmt_minutes(overall_end_min_min),
                "gio_ket_thuc_muon_nhat": _fmt_minutes(overall_end_min_max),
                "tb_so_gio_lam_ngay": None if overall_hours_mean is None else round(float(overall_hours_mean), 2),
                "so_tin": overall_count,
            }

            glv_out = pd.concat([glv_stats, pd.DataFrame([total_row])], ignore_index=True)

            # 4) Ghi Excel (sheet: 'gio_lam_viec'); n·∫øu sheet t·ªìn t·∫°i s·∫Ω replace
            try:
                with pd.ExcelWriter(out_phantich, engine="openpyxl", mode="a", if_sheet_exists="replace") as writer:
                    glv_out.to_excel(writer, sheet_name="gio_lam_viec", index=False)
                print(f"üìÑ ƒê√£ l∆∞u {len(glv_out)} d√≤ng (sheet: 'gio_lam_viec').")
            except Exception as e:
                print(f"‚ùå L·ªói khi ghi Excel (gio_lam_viec): {e}")
            # =========================================================================
            # ==== 13) Ph√¢n t√≠ch k·ªπ nƒÉng (sheet: ky_nang & ky_nang_theo_nganh) ====
            from sentence_transformers import SentenceTransformer, util
            SKILL_COL = "ky_nang"
            TARGET_COL = "nganh"

            # T·∫≠p k√Ω t·ª± c√≥ d·∫•u ti·∫øng Vi·ªát (ƒë·ªß r·ªông; kh√¥ng c·∫ßn li·ªát k√™ ƒë·∫ßy ƒë·ªß)
            _VN_DIACRITICS = set("ƒÉ√¢ƒë√™√¥∆°∆∞√°√†·∫£√£·∫°·∫Ø·∫±·∫≥·∫µ·∫∑·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π·∫ø·ªÅ·ªÉ·ªÖ·ªá√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç·ªë·ªì·ªï·ªó·ªô·ªõ·ªù·ªü·ª°·ª£√∫√π·ªß≈©·ª•·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµ")

            def _has_vn_diacritic(s: str) -> bool:
                # C√≥ ch·ª©a b·∫•t k·ª≥ k√Ω t·ª± c√≥ d·∫•u ti·∫øng Vi·ªát?
                return any(ch.lower() in _VN_DIACRITICS for ch in s)

            def _has_cjk(s: str) -> bool:
                # C√≥ k√Ω t·ª± CJK Unified Ideographs (Trung/Nh·∫≠t/H√†n)?
                return any('\u4e00' <= ch <= '\u9fff' for ch in s)

            _ASCII_LETTERS_RE = re.compile(r"[A-Za-z]")

            def _lang_rank(s: str) -> int:
                # ∆Øu ti√™n ng√¥n ng·ªØ ƒë·∫°i di·ªán khi ch·ªçn "canon" cho c·ª•m ƒë·ªìng nghƒ©a:
                # 0 = English (ASCII & c√≥ √≠t nh·∫•t 1 ch·ªØ c√°i), 1 = Vietnamese (c√≥ d·∫•u), 2 = Others
                if not s:
                    return 2
                if all(ord(ch) < 128 for ch in s) and _ASCII_LETTERS_RE.search(s):
                    return 0
                if _has_vn_diacritic(s):
                    return 1
                return 2

            # ---------- Helpers chu·∫©n ho√° ----------
            def _no_accent_lower(s: str) -> str:
                # B·ªè d·∫•u + lower + trim (d√πng cho m·ªôt s·ªë so kh·ªõp m·ªÅm)
                if s is None:
                    return ""
                s = unicodedata.normalize("NFD", str(s))
                s = "".join(ch for ch in s if unicodedata.category(ch) != "Mn")
                return s.strip().lower()

            # C√°c k·ªπ nƒÉng ng·∫Øn nh∆∞ng h·ª£p l·ªá v·∫´n gi·ªØ (tr√°nh l·ªçc nh·∫ßm)
            ALLOWED_SHORT = {
                "c", "r", "go", "qa", "ui", "ux", "ai", "bi", "qa/qc", "ci", "cd", "git",
            }

            # Chu·∫©n ho√° m·ªôt s·ªë bi·∫øn th·ªÉ ph·ªï bi·∫øn ƒë·ªÉ gom v·ªÅ d·∫°ng chung
            def _normalize_form(s: str) -> str:
                s = s.replace("cyber security", "cybersecurity")
                s = s.replace("information security", "infosec")
                s = s.replace("ux ui", "ux/ui")
                s = s.replace("ms office", "microsoft office")
                s = s.replace("bao mat thong tin", "an toan thong tin")
                s = s.replace("an ninh thong tin", "an ninh mang")
                return s

            # Lo·∫°i b·ªè token r√°c / v√¥ nghƒ©a (to√†n d·∫•u, to√†n s·ªë, 1 k√Ω t·ª±, ...)
            _punct_re = re.compile(r"^[\W_]+$")

            def is_valid_skill(tok: str) -> bool:
                if not tok:
                    return False
                if tok in ALLOWED_SHORT:
                    return True
                if len(tok) == 1:
                    return False
                if tok.isdigit():
                    return False
                if _punct_re.match(tok):
                    return False
                return True

            # ---------- Ki·ªÉm tra c·ªôt ngu·ªìn ----------
            if SKILL_COL not in df.columns or TARGET_COL not in df.columns:
                print(f"‚ö†Ô∏è B·ªè qua ph√¢n t√≠ch k·ªπ nƒÉng (thi·∫øu c·ªôt '{SKILL_COL}' ho·∫∑c '{TARGET_COL}').")
            else:
                try:
                    print("üîé ƒêang x·ª≠ l√Ω k·ªπ nƒÉng...")

                    # C·ªë g·∫Øng n·∫°p sentence-transformers ƒë·ªÉ gom c·ª•m ƒë·ªìng nghƒ©a; n·∫øu kh√¥ng ƒë∆∞·ª£c s·∫Ω fallback
                    model = None
                    util = None
                    try:
                        from sentence_transformers import SentenceTransformer, util as _util
                        model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
                        util = _util
                        print("‚úÖ D√πng sentence-transformers ƒë·ªÉ gom c·ª•m ƒë·ªìng nghƒ©a.")
                    except Exception as _e:
                        print(
                            f"‚ÑπÔ∏è Kh√¥ng d√πng ƒë∆∞·ª£c sentence-transformers (s·∫Ω gom theo tr√πng kh·ªõp ch√≠nh x√°c). L√Ω do: {_e}")

                    # 1) Chu·∫©n ho√° & t√°ch k·ªπ nƒÉng theo c√°c d·∫•u ph√¢n t√°ch th√¥ng d·ª•ng
                    skill_rows = []
                    splitter = re.compile(r"[,;/\|\n]+")
                    for _, row in df.iterrows():
                        nganh_val = str(row.get(TARGET_COL, "")).strip() or "no_info"
                        raw = "" if pd.isna(row.get(SKILL_COL)) else str(row.get(SKILL_COL))

                        # Chu·∫©n ho√° nh·∫π (kh√¥ng b·ªè d·∫•u ƒë·ªÉ gi·ªØ nguy√™n t√™n g·ªëc n·∫øu c·∫ßn)
                        raw_norm = _normalize_form(raw.lower())
                        skills = [s.strip() for s in splitter.split(raw_norm) if s.strip()]

                        for sk in skills:
                            if not is_valid_skill(sk):
                                continue
                            skill_rows.append((nganh_val, sk))

                    if not skill_rows:
                        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y k·ªπ nƒÉng h·ª£p l·ªá ƒë·ªÉ ph√¢n t√≠ch.")
                    else:
                        # 2) Danh s√°ch k·ªπ nƒÉng duy nh·∫•t (b·∫£o to√†n th·ª© t·ª± g·∫∑p ƒë·∫ßu ti√™n)
                        all_skills = [s for _, s in skill_rows]
                        unique_skills = list(dict.fromkeys(all_skills))

                        # 3) Gom c·ª•m ƒë·ªìng nghƒ©a (n·∫øu c√≥ model); ng∆∞·ª£c l·∫°i: m·ªói skill t·ª± l√† ƒë·∫°i di·ªán
                        if model is not None and util is not None and len(unique_skills) > 1:
                            emb = model.encode(unique_skills, convert_to_tensor=True, show_progress_bar=False)

                            clusters = {}
                            used = set()
                            THRESH = 0.80  # ng∆∞·ª°ng cosine similarity ƒë·ªÉ gom c·ª•m
                            for i, sk in enumerate(unique_skills):
                                if i in used:
                                    continue
                                group = [sk]
                                used.add(i)
                                for j in range(i + 1, len(unique_skills)):
                                    if j in used:
                                        continue
                                    # ƒê·ªô t∆∞∆°ng ƒë·ªìng cosine gi·ªØa embedding i v√† j
                                    if util.cos_sim(emb[i], emb[j]).item() >= THRESH:
                                        group.append(unique_skills[j])
                                        used.add(j)
                                # Ch·ªçn ƒë·∫°i di·ªán (canon) theo: ∆∞u ti√™n ng√¥n ng·ªØ ‚Üí ƒë·ªô d√†i ‚Üí th·ª© t·ª± alpha
                                canon = sorted(
                                    group,
                                    key=lambda x: (_lang_rank(x), len(x), x)
                                )[0]

                                clusters[canon] = group

                            skill2canon = {g: canon for canon, group in clusters.items() for g in group}
                        else:
                            # Fallback: kh√¥ng gom; canon = ch√≠nh k·ªπ nƒÉng ƒë√≥
                            skill2canon = {s: s for s in unique_skills}

                        # 4) B·∫£ng ƒë·∫øm (ng√†nh, k·ªπ nƒÉng_ƒë·∫°i_di·ªán)
                        df_skill = (
                            pd.DataFrame([(ng, skill2canon.get(sk, sk)) for ng, sk in skill_rows],
                                         columns=["nganh", "skill_list"])
                            .groupby(["nganh", "skill_list"], dropna=False)
                            .size()
                            .reset_index(name="so_tin")
                        )

                        # ---------------- A) Sheet: ky_nang ----------------
                        # T·∫°o c·ªôt top10 k·ªπ nƒÉng nhi·ªÅu nh·∫•t & √≠t nh·∫•t theo t·ª´ng ng√†nh (li·ªát k√™ t√™n k·ªπ nƒÉng)
                        def _top_join(sub_df: pd.DataFrame, ascending=False, k=10) -> str:
                            if sub_df.empty:
                                return ""
                            srt = sub_df.sort_values(["so_tin", "skill_list"],
                                                     ascending=[ascending, True],
                                                     ignore_index=True)
                            pick = srt.head(k) if ascending else srt.tail(k)
                            pick = pick.sort_values(["so_tin", "skill_list"], ascending=[False, True])
                            return ", ".join(pick["skill_list"].tolist())

                        per_nganh = []
                        for ng, sub in df_skill.groupby("nganh", dropna=False):
                            most10 = _top_join(sub, ascending=False, k=10)
                            least10 = _top_join(sub, ascending=True, k=10)
                            per_nganh.append({
                                "nganh": ng,
                                "top10_ky_nang_nhieu_nhat": most10,
                                "top10_ky_nang_it_nhat": least10
                            })
                        ky_nang_df = pd.DataFrame(per_nganh)

                        # ---------------- B) Sheet: ky_nang_theo_nganh ----------------
                        # L·∫•y 10 k·ªπ nƒÉng ph·ªï bi·∫øn nh·∫•t to√†n c·ª•c r·ªìi pivot theo ng√†nh
                        top10_global = (
                            df_skill.groupby("skill_list", dropna=False)["so_tin"]
                            .sum()
                            .sort_values(ascending=False)
                            .head(10)
                            .index
                            .tolist()
                        )

                        # Pivot: index = ng√†nh, columns = k·ªπ nƒÉng ƒë·∫°i di·ªán, values = s·ªë tin
                        pivot = (
                            df_skill.pivot_table(
                                index="nganh",
                                columns="skill_list",
                                values="so_tin",
                                aggfunc="sum",
                                fill_value=0
                            )
                            .reindex(columns=top10_global, fill_value=0)
                            .reset_index()
                        )
                        pivot.columns.name = None
                        ky_nang_theo_nganh_df = pivot

                        # 5) Ghi Excel: 2 sheet 'ky_nang' v√† 'ky_nang_theo_nganh' (replace n·∫øu ƒë√£ c√≥)
                        out_path = Path(out_phantich)
                        out_path.parent.mkdir(parents=True, exist_ok=True)

                        mode = "a" if out_path.exists() else "w"
                        try:
                            with pd.ExcelWriter(out_path, engine="openpyxl", mode=mode,
                                                if_sheet_exists="replace") as writer:
                                ky_nang_df.to_excel(writer, sheet_name="ky_nang", index=False)
                                ky_nang_theo_nganh_df.to_excel(writer, sheet_name="ky_nang_theo_nganh", index=False)
                            print(f"üìÑ ƒê√£ l∆∞u {len(ky_nang_df)} d√≤ng (sheet: 'ky_nang').")
                            print(f"üìÑ ƒê√£ l∆∞u {len(ky_nang_theo_nganh_df)} d√≤ng (sheet: 'ky_nang_theo_nganh').")
                        except Exception as e:
                            print(f"‚ùå L·ªói khi ghi Excel (sheet ky_nang / ky_nang_theo_nganh): {e}")

                except Exception as e:
                    # B·∫Øt m·ªçi l·ªói trong pipeline x·ª≠ l√Ω k·ªπ nƒÉng ƒë·ªÉ kh√¥ng l√†m g√£y to√†n b·ªô lu·ªìng
                    print(f"‚ùå L·ªói khi x·ª≠ l√Ω k·ªπ nƒÉng: {e}")
    #==============================================================================
    # ==== 14) Th·ªëng k√™ nh√≥m ph√∫c l·ª£i theo ng√†nh (ghi sheet: phuc_loi_nhom_theo_nganh) ====
    TARGET_COL = "nganh"
    BENEFIT_GROUP_COL = "nhom_phuc_loi"
    SHEET_NAME = "phuc_loi_nhom_theo_nganh"

    # Y√™u c·∫ßu: ph·∫£i c√≥ c·ªôt ng√†nh v√† c·ªôt "nh√≥m ph√∫c l·ª£i" (d·∫°ng text, c√≥ th·ªÉ ch·ª©a nhi·ªÅu nh√≥m, ngƒÉn c√°ch b·∫±ng "|").
    if TARGET_COL not in df.columns or BENEFIT_GROUP_COL not in df.columns:
        print(f"‚ö†Ô∏è B·ªè qua th·ªëng k√™ ph√∫c l·ª£i: thi·∫øu c·ªôt '{TARGET_COL}' ho·∫∑c '{BENEFIT_GROUP_COL}'.")
    else:
        splitter_groups = re.compile(r"\s*\|\s*")  # B·ªô t√°ch c√°c nh√≥m trong m·ªôt √¥, cho ph√©p kho·∫£ng tr·∫Øng hai b√™n

        # L·∫•y ph·∫ßn TR∆Ø·ªöC d·∫•u ":" l√†m t√™n nh√≥m chu·∫©n.
        # V√≠ d·ª• √¥: "Luong-Thuong: Bonus, Thuong hieu suat/KPI | PhucLoiKhac: B·∫£o hi·ªÉm m·ªü r·ªông"
        # -> groups = ["Luong-Thuong", "PhucLoiKhac"]
        def parse_groups(cell: str):
            if not isinstance(cell, str) or not cell.strip():
                return []
            groups_raw = splitter_groups.split(cell.strip())
            groups = []
            for part in groups_raw:
                if not part:
                    continue
                # V√≠ d·ª•: "Luong-Thuong: Bonus, Thuong hieu suat/KPI"
                g = part.split(":", 1)[0].strip()
                if not g:
                    continue
                groups.append(g)
            # Lo·∫°i tr√πng l·∫∑p trong C√ôNG m·ªôt b·∫£n ghi (b·∫£o to√†n th·ª© t·ª± g·∫∑p ƒë·∫ßu ti√™n)
            return list(dict.fromkeys(groups))

        # Tr·∫£i ph·∫≥ng d·ªØ li·ªáu: m·ªói (ng√†nh, nh√≥m) l√† m·ªôt d√≤ng ƒë·ªÉ ƒë·∫øm
        rows = []
        for _, r in df.iterrows():
            nganh_val = str(r.get(TARGET_COL, "")).strip() or "no_info"
            gs = parse_groups(r.get(BENEFIT_GROUP_COL, ""))
            for g in gs:
                rows.append((nganh_val, g))

        if not rows:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ph√∫c l·ª£i ƒë·ªÉ th·ªëng k√™.")
        else:
            # ƒê·∫øm s·ªë tin theo (ng√†nh, nh√≥m ph√∫c l·ª£i)
            df_benefit = (
                pd.DataFrame(rows, columns=["nganh", "nhom_phuc_loi"])
                .groupby(["nganh", "nhom_phuc_loi"], dropna=False)
                .size()
                .reset_index(name="so_tin")
            )

            # Pivot r·ªông: h√†ng = ng√†nh, c·ªôt = nh√≥m ph√∫c l·ª£i, √¥ = s·ªë tin (fill 0)
            pivot = (
                df_benefit.pivot_table(
                    index="nganh",
                    columns="nhom_phuc_loi",
                    values="so_tin",
                    aggfunc="sum",
                    fill_value=0
                )
                .reset_index()
            )
            pivot.columns.name = None  # B·ªè t√™n tr·ª•c c·ªôt do pivot t·∫°o ra

            # Ghi sheet ra Excel (append n·∫øu file ƒë√£ t·ªìn t·∫°i; replace sheet n·∫øu ƒë√£ c√≥ c√πng t√™n)
            out_path = Path(out_phantich)
            out_path.parent.mkdir(parents=True, exist_ok=True)
            mode = "a" if out_path.exists() else "w"
            try:
                with pd.ExcelWriter(out_path, engine="openpyxl", mode=mode, if_sheet_exists="replace") as w:
                    pivot.to_excel(w, sheet_name=SHEET_NAME, index=False)
                print(f"üìÑ ƒê√£ l∆∞u {len(pivot)} d√≤ng (sheet: '{SHEET_NAME}').")
            except Exception as e:
                print(f"‚ùå L·ªói khi ghi Excel (sheet {SHEET_NAME}): {e}")
    #=====================================================================
    # ==== 15) ƒêi·ªÅn khuy·∫øt to√†n b·ªô c√°c sheet: null/NaN/chu·ªói r·ªóng -> "Kh√¥ng c√≥ th√¥ng tin" ====
    try:
        out_path = Path(out_phantich)
        if not out_path.exists():
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file ph√¢n t√≠ch ƒë·ªÉ ƒëi·ªÅn khuy·∫øt: {out_path}")
        else:
            # ƒê·ªçc to√†n b·ªô sheets, gi·ªØ nguy√™n th·ª© t·ª±
            sheets_dict = pd.read_excel(out_path, sheet_name=None, engine="openpyxl")
            if not sheets_dict:
                print("‚ö†Ô∏è File kh√¥ng c√≥ sheet n√†o.")
            else:
                total_cells_filled = 0
                processed = {}

                def _fill_df(df: pd.DataFrame) -> Tuple[pd.DataFrame, int]:
                    """Tr·∫£ v·ªÅ (df_moi, so_o_duoc_dien)."""
                    if df is None or df.empty:
                        # N·∫øu sheet tr·ªëng -> t·∫°o 1 √¥ th√¥ng b√°o
                        return pd.DataFrame({"Th√¥ng tin": ["Kh√¥ng c√≥ th√¥ng tin"]}), 1

                    # ƒê·∫øm √¥ thi·∫øu (NaN/None) ban ƒë·∫ßu
                    na_before = df.isna().sum().sum()

                    # Chu·∫©n ho√°: chu·ªói r·ªóng/white-space -> NaN
                    def _norm_empty(x):
                        return np.nan if (isinstance(x, str) and x.strip() == "") else x

                    df2 = df.applymap(_norm_empty)

                    # ƒêi·ªÅn khuy·∫øt m·ªçi NaN/None c√≤n l·∫°i
                    df2 = df2.fillna("Kh√¥ng c√≥ th√¥ng tin")

                    # S·ªë √¥ ƒë∆∞·ª£c ƒëi·ªÅn = NaN ban ƒë·∫ßu + s·ªë √¥ l√† chu·ªói r·ªóng tr∆∞·ªõc ƒë√≥
                    # (x·∫•p x·ªâ: ƒë·∫øm l·∫°i s·ªë √¥ "Kh√¥ng c√≥ th√¥ng tin" tr·ª´ ƒëi s·ªë √¥ ƒë√£ c√≥ gi√° tr·ªã n√†y t·ª´ tr∆∞·ªõc)
                    # ƒê∆°n gi·∫£n h∆°n: ∆∞·ªõc l∆∞·ª£ng b·∫±ng na_before + empty_count
                    empty_count = 0
                    if not df.empty:
                        # ƒê·∫øm chu·ªói r·ªóng ban ƒë·∫ßu
                        empty_count = sum(
                            1
                            for col in df.columns
                            for v in df[col].tolist()
                            if isinstance(v, str) and v.strip() == ""
                        )
                    filled = int(na_before + empty_count)
                    return df2, filled

                for sheet_name, df_sheet in sheets_dict.items():
                    df_filled, filled_cnt = _fill_df(df_sheet)
                    processed[sheet_name] = df_filled
                    total_cells_filled += filled_cnt

                # Ghi ƒë√® to√†n b·ªô file, gi·ªØ t√™n sheet nh∆∞ c≈©
                with pd.ExcelWriter(out_path, engine="openpyxl", mode="w") as w:
                    for sheet_name, df_sheet in processed.items():
                        df_sheet.to_excel(w, sheet_name=sheet_name, index=False)

                print(f"üß© ƒê√£ ƒëi·ªÅn khuy·∫øt to√†n b·ªô file: {out_path.name}")
                print(f"   ‚Ä¢ S·ªë sheet: {len(processed)}")
                print(f"   ‚Ä¢ S·ªë √¥ ƒë∆∞·ª£c ƒëi·ªÅn (∆∞·ªõc l∆∞·ª£ng): {total_cells_filled}")
    except Exception as e:
        print(f"‚ùå L·ªói ·ªü b∆∞·ªõc ƒëi·ªÅn khuy·∫øt (m·ª•c 15): {e}")

    # K·∫øt th√∫c ph√¢n t√≠ch ‚Üí tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n file Excel t·ªïng h·ª£p
    return out_phantich

def main():
    # N·∫øu ƒë·∫∑t bi·∫øn m√¥i tr∆∞·ªùng EXCEL_PATH_ANALYZER ‚Üí ch·∫°y ph√¢n t√≠ch cho ƒë√∫ng file ƒë√≥
    file_path_env = os.getenv("EXCEL_PATH_ANALYZER")
    if file_path_env:
        src = Path(file_path_env)
        out = analyze_one_file(src)
        print(f"‚úÖ Ho√†n t·∫•t: {out}")
        return

    # N·∫øu kh√¥ng c√≥ ENV ‚Üí t·ª± ƒë·ªông qu√©t c√°c file chi ti·∫øt m·ªõi nh·∫•t trong th∆∞ m·ª•c preprocess v√† x·ª≠ l√Ω tu·∫ßn t·ª±
    files = get_latest_detail_files(PREPROCESS_DIR)
    if not files:
        raise SystemExit(f"‚ùå Kh√¥ng t√¨m th·∫•y file n√†o trong {PREPROCESS_DIR}")

    print(f"[INFO] S·∫Ω x·ª≠ l√Ω {len(files)} file:")
    for f in files:
        print(" -", f.name)

    ok, fail = 0, 0
    for fp in files:
        try:
            out = analyze_one_file(fp)
            print(f"‚úÖ Ho√†n t·∫•t: {out}")
            ok += 1
        except Exception as e:
            print(f"‚ùå L·ªói khi x·ª≠ l√Ω {fp.name}: {e}")
            fail += 1

    # T·ªïng k·∫øt k·∫øt qu·∫£ ch·∫°y batch
    print("\n========== T·ªîNG K·∫æT ==========")
    print(f"‚úîÔ∏è Th√†nh c√¥ng: {ok}")
    print(f"‚ùå Th·∫•t b·∫°i : {fail}")


if __name__ == "__main__":
    main()

